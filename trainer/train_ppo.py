import os
import sys

__package__ = "trainer"
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import argparse
import re
import warnings
import torch
import torch.distributed as dist
import torch.nn.functional as F
from transformers import AutoTokenizer
from contextlib import nullcontext
from torch import optim, nn
from torch.nn.parallel import DistributedDataParallel
from torch.utils.data import DataLoader, DistributedSampler
from torch.nn.utils import clip_grad_norm_
from torch.optim.lr_scheduler import CosineAnnealingLR
from transformers import AutoModel
from model.model_minimind import MiniMindConfig, MiniMindForCausalLM
from dataset.lm_dataset import RLAIFDataset
from trainer.train_utils import Logger, is_main_process, lm_checkpoint, init_distributed_mode, setup_seed, SkipBatchSampler, init_model

warnings.filterwarnings('ignore')

# ç¦ç”¨Flash Attention
import torch.backends.cuda
# å¼ºåˆ¶ä½¿ç”¨çº¯æ•°å­¦è®¡ç®—ï¼Œè™½ç„¶æ…¢ï¼Œä½†æœ€ç¨³
torch.backends.cuda.enable_flash_sdp(False)
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_math_sdp(True)

class CriticModel(MiniMindForCausalLM):
    def __init__(self, config):
        super().__init__(config)
        # æ›¿æ¢lm_headä¸ºè¾“å‡ºå•ä¸€ä»·å€¼çš„çº¿æ€§å±‚
        self.value_head = nn.Linear(config.hidden_size, 1)

    def forward(self, input_ids=None, attention_mask=None, **kwargs):
        # ä½¿ç”¨åŸºç¡€æ¨¡å‹è·å–éšè—çŠ¶æ€
        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)
        hidden_states = self.model.norm(outputs[0])
        # ä½¿ç”¨value_headè·å–ä»·å€¼ä¼°è®¡
        values = self.value_head(hidden_states).squeeze(-1)
        return values

def calculate_rewards(prompts, responses, reward_model, reward_tokenizer):
    """æ•´åˆæ‰€æœ‰å¥–åŠ±å‡½æ•°è®¡ç®—æ€»å¥–åŠ±"""
    def reasoning_model_reward(rewards):
        # 1. æ ¼å¼å¥–åŠ±ï¼ˆä»…é’ˆå¯¹è®­ç»ƒæ¨ç†æ¨¡å‹æ—¶ä½¿ç”¨ï¼‰
        pattern = r"^<think>\n.*?\n</think>\n<answer>\n.*?\n</answer>$"
        pattern2 = r"^<think>\n.*?\n</think>\n\n<answer>\n.*?\n</answer>$"

        matches_pattern = [re.match(pattern, response, re.S) for response in responses]
        matches_pattern2 = [re.match(pattern2, response, re.S) for response in responses]

        format_rewards = []
        for match_pattern, match_pattern2 in zip(matches_pattern, matches_pattern2):
            if match_pattern:
                format_rewards.append(0.5)
            elif match_pattern2:
                format_rewards.append(0.5)
            else:
                format_rewards.append(0.0)
        rewards += torch.tensor(format_rewards, device=args.device)

        # 2. æ ‡è®°å¥–åŠ±ï¼ˆé˜²æ­¢ä¸¥æ ¼å¥–åŠ±ç¨€ç–ï¼Œä»…é’ˆå¯¹è®­ç»ƒæ¨ç†æ¨¡å‹æ—¶ä½¿ç”¨ï¼‰
        def mark_num(text):
            reward = 0
            if text.count("<think>") == 1:
                reward += 0.25
            if text.count("</think>") == 1:
                reward += 0.25
            if text.count("<answer>") == 1:
                reward += 0.25
            if text.count("</answer>") == 1:
                reward += 0.25
            return reward

        mark_rewards = [mark_num(response) for response in responses]
        rewards += torch.tensor(mark_rewards, device=args.device)
        return rewards

    rewards = torch.zeros(len(responses), device=args.device)

    # æ ¼å¼å¥–åŠ±
    if args.reasoning == 1:
        rewards = reasoning_model_reward(rewards)

    # ä½¿ç”¨reward modelè®¡ç®—æ•´ä¸ªresponseçš„å¥–åŠ±
    with torch.no_grad():
        reward_model_scores = []
        for prompt, response in zip(prompts, responses):
            pattern = r"<\|im_start\|>(system|user|assistant)\s+(.*?)<\|im_end\|>"
            matches = re.findall(pattern, prompt, re.DOTALL)
            messages = [{"role": role, "content": content.strip()} for role, content in matches]

            tmp_chat = messages + [{"role": "assistant", "content": response}]
            score = reward_model.get_score(reward_tokenizer, tmp_chat)

            scale = 3.0
            score = max(min(score, scale), -scale)

            # å½“args.reasoning=1æ—¶ï¼Œé¢å¤–è®¡ç®—<answer>å†…å®¹çš„å¥–åŠ±
            if args.reasoning == 1:
                answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)
                if answer_match:
                    answer_content = answer_match.group(1).strip()
                    # å¯¹answerå†…å®¹å•ç‹¬è®¡ç®—reward
                    tmp_chat = messages + [{"role": "assistant", "content": answer_content}]
                    answer_score = reward_model.get_score(reward_tokenizer, tmp_chat)
                    answer_score = max(min(answer_score, scale), -scale)
                    score = score * 0.4 + answer_score * 0.6
            reward_model_scores.append(score)

        reward_model_scores = torch.tensor(reward_model_scores, device=args.device)
        rewards += reward_model_scores

    return rewards


def ppo_train_epoch(epoch, loader, iters, old_actor_model, ref_model, actor_scheduler, critic_scheduler, reward_model, reward_tokenizer, start_step=0, wandb=None):
    actor_model.train()
    critic_model.train()

    for step, batch in enumerate(loader, start=start_step + 1):
        prompts = batch["prompt"]  # list[str], length B
        enc = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, # enc.shape: [Batch_Size, batch_max_seq_Len] 
                       max_length=args.max_seq_len).to(args.device)  # input_ids: [B, P], attention_mask: [B, P]

#Prefix: ======= æ’å…¥è¿™æ®µè°ƒè¯•ä»£ç  =======
        print(f"\n[DEBUG Step]")
        print(f"Input shape: {enc.input_ids.shape}")
        print(f"Max ID in input: {enc.input_ids.max().item()}")
        print(f"Min ID in input: {enc.input_ids.min().item()}")
        print(f"Pad Token ID: {tokenizer.pad_token_id}")
        print(f"EOS Token ID: {tokenizer.eos_token_id}")
        print(f"Model config vocab limit: {actor_model.config.vocab_size}")

        # ä¸¥æŸ¥è¶Šç•Œ
        if enc.input_ids.max().item() >= actor_model.config.vocab_size:
            print("ğŸ”´ CRITICAL ERROR: Input ID exceeds model vocabulary size!")
            print(f"Found ID {enc.input_ids.max().item()} >= {actor_model.config.vocab_size}")
            print("è¿™ä¼šå¯¼è‡´ CUDA error: device-side assert triggered")
            print("è¯·æ£€æŸ¥ Tokenizer æ˜¯å¦è¾“å‡ºäº† 6400ï¼Ÿå¦‚æœæ˜¯ï¼Œä½ éœ€è¦æŠŠæ¨¡å‹çš„ vocab_size è®¾ä¸º 6401 æˆ–æ›´å¤§ã€‚")
            import sys; sys.exit(1)
            
        # ä¸¥æŸ¥ NaN æƒé‡ (é˜²æ­¢åŠ è½½çš„æƒé‡æœ¬èº«å°±æ˜¯åçš„)
        for name, param in actor_model.named_parameters():
            if torch.isnan(param).any():
                print(f"ğŸ”´ CRITICAL ERROR: Parameter {name} contains NaN!")
                import sys; sys.exit(1)

        print(f"Attention Mask Shape: {enc.attention_mask.shape}")
        # æ£€æŸ¥æ¯ä¸€è¡Œ mask çš„å’Œ
        mask_sums = enc.attention_mask.sum(dim=1)
        print(f"Mask Sums per row: {mask_sums}")
        
        if (mask_sums == 0).any():
            print("ğŸ”´ CRITICAL ERROR: Found a row with ALL-ZERO attention mask!")
            print("Reason: One of your prompts is empty or fully filtered out by tokenizer.")
            print("Solution: Check your dataset/jsonl file for empty strings.")
            import sys; sys.exit(1)
            
        # è¿›ä¸€æ­¥æ£€æŸ¥ï¼šæ˜¯ä¸æ˜¯åªæœ‰ <bos> æ²¡æœ‰å…¶ä»–å†…å®¹ï¼Ÿ
        # å¦‚æœ mask sum == 1 (åªæœ‰ bos)ï¼Œæœ‰æ—¶å€™ä¹Ÿä¼šå¯¼è‡´åç»­è®¡ç®—ä¸ç¨³å®š
        if (mask_sums <= 1).any():
            print("âš ï¸ WARNING: Found a row with extremely short prompt (length <= 1).")
            print("è¿™å¯èƒ½å¯¼è‡´ Attention è®¡ç®—ä¸ç¨³å®šã€‚")
#Prefix: ==================================
        # torch.full((B,), L): åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸º Batch Size (B) çš„å‘é‡ï¼Œé‡Œé¢çš„æ¯ä¸ªå€¼éƒ½æ˜¯ Lã€‚
        # é…åˆ left padding ä½¿ç”¨ï¼Œè¡¨ç¤ºæ¯ä¸ªåºåˆ—çš„å®é™…å†…å®¹é•¿åº¦ï¼ˆåŒ…å«paddingï¼‰ï¼Œæ–¹ä¾¿åç»­æ‰¾å‡ºç”Ÿæˆå†…å®¹çš„èµ·å§‹ç´¢å¼•ã€‚
        prompt_lengths = torch.full((enc.input_ids.size(0),), enc.input_ids.shape[1], dtype=torch.long, device=enc.input_ids.device)  # [B]

# === æ’å…¥æµ‹è¯•ä»£ç  ===
        print("[Debug] Testing forward pass before generate...")
        with torch.no_grad():
            # æ‰‹åŠ¨è·‘ä¸€æ¬¡å‰å‘ä¼ æ’­
            model_for_gen = actor_model.module if isinstance(actor_model, DistributedDataParallel) else actor_model
            test_out = model_for_gen(input_ids=enc.input_ids, attention_mask=enc.attention_mask)
            test_logits = test_out.logits
            
            if torch.isnan(test_logits).any():
                print("âŒ Forward pass produced NaN logits!")
                print(f"Logits max: {test_logits.max()}, min: {test_logits.min()}")
                
                # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å“ªä¸€å±‚å‡ºçš„é—®é¢˜ï¼ˆå¦‚æœæœ‰Embeddingè¾“å‡ºNaNï¼Œé‚£å°±æ˜¯Embeddingçš„é—®é¢˜ï¼‰
                # è¿™é‡Œå‡è®¾ä½ æœ‰åŠæ³•è®¿é—® embeddingsï¼Œé€šå¸¸æ˜¯:
                # print("Embed out:", model_for_gen.model.embed_tokens(enc.input_ids))
                exit(1)
            else:
                print("âœ… Forward pass is clean. Logits are finite.")
# ===================

        '''Step 1: é‡‡æ · (Rollout)'''
        with torch.no_grad():
            # å¦‚æœ actor_model æ˜¯ DDP åŒ…è£…è¿‡çš„ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡ actor_model.module è®¿é—®å†…éƒ¨çœŸæ­£çš„æ¨¡å‹ï¼Œæ‰èƒ½è°ƒç”¨ .generate()ã€‚
            model_for_gen = actor_model.module if isinstance(actor_model, DistributedDataParallel) else actor_model
            gen_out = model_for_gen.generate(
                input_ids=enc.input_ids,          # Prompt çš„ Token ID
                attention_mask=enc.attention_mask,# Prompt çš„æ©ç 
                max_new_tokens=args.max_gen_len,  # åªé™åˆ¶æ–°ç”Ÿæˆçš„ Token æ•°é‡ (Responseé•¿åº¦)
                do_sample=True,                   # å¼€å¯é‡‡æ · (Sampling)
                temperature=0.8,                  # æ¸©åº¦ç³»æ•°
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        '''Step 2: è®¡ç®—å¥–åŠ±ä¸ä»·å€¼'''
        responses_text = [tokenizer.decode(gen_out[i, prompt_lengths[i]:], skip_special_tokens=True) for i in range(len(prompts))]
        rewards = calculate_rewards(prompts, responses_text, reward_model, reward_tokenizer)  # [B]
        
        full_mask = (gen_out != tokenizer.pad_token_id).long()  # [B, P+R]
        values_seq = critic_model(input_ids=gen_out, attention_mask=full_mask)  # [B, P+R]
        last_indices = (full_mask * torch.arange(full_mask.size(1), device=gen_out.device)).argmax(dim=1)
        values = values_seq[torch.arange(values_seq.size(0), device=values_seq.device), last_indices]  # [B]

        '''Step 3: è®¡ç®—ä¼˜åŠ¿å‡½æ•° (Advantage Estimation)'''
        advantages = rewards - values.detach()  # [B]        

        '''Step 4: è®¡ç®—å¯¹æ•°æ¦‚ç‡ (Log Probabilities)
            1. å½“å‰ç­–ç•¥æ¦‚ç‡ï¼šactor_logp
            2. æ—§ç­–ç•¥æ¦‚ç‡ï¼šold_logp
            3. å‚è€ƒç­–ç•¥æ¦‚ç‡ï¼šref_logp
        '''
        # gen_out.shape: [batch_size, seq_len]
        # logits.shape: [batch_size, seq_len, vocab_size]
        logits = actor_model(input_ids=gen_out, attention_mask=full_mask).logits  # [B, P+R, V]
        # labels.shape: [batch_size, seq_len - 1]
        labels = gen_out[:, 1:].clone()  # [B, P+R-1]
        # logits[:, :-1].shape: [batch_size, seq_len - 1, vocab_size] å»æ‰æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„logitsï¼Œå› ä¸ºæ²¡æœ‰å¯¹åº”çš„æ ‡ç­¾
        # F.log_softmax(logits[:, :-1], dim=-1).shape: [batch_size, seq_len - 1([token_id]), vocab_size(log_prob)]
        # labels.unsqueeze(-1).shape: [batch_size, seq_len - 1, 1(token_id)]
        # F.log_softmax(logits[:, :-1], dim=-1).gather(2, labels.unsqueeze(-1)).shape: [batch_size, seq_len - 1, 1(log_prob)]
        # logp_tokens.shape: [batch_size, seq_len - 1([log_prob])]
        logp_tokens = F.log_softmax(logits[:, :-1], dim=-1).gather(2, labels.unsqueeze(-1)).squeeze(-1)  # [B, P+R-1]
        seq_len = gen_out.size(1) - 1
        # å°†éresponseéƒ¨åˆ†çš„log_probå±è”½æ‰
        resp_mask = torch.arange(seq_len, device=gen_out.device).unsqueeze(0) >= prompt_lengths.unsqueeze(1)
        # å°†éresponseå’Œpaddingéƒ¨åˆ†çš„log_probå±è”½æ‰ï¼Œè·å¾—æœ€ç»ˆçš„final_mask
        final_mask = resp_mask & (~labels.eq(tokenizer.pad_token_id))  # [B, P+R-1]
        # å¯¹æ‰€æœ‰ç”Ÿæˆçš„æœ‰æ•ˆçš„log_probæ±‚å’Œï¼Œè·å¾—æœ€ç»ˆçš„actor_logp
        actor_logp = (logp_tokens * final_mask).sum(dim=1)  # [B]

        with torch.no_grad():
            old_logits = old_actor_model(input_ids=gen_out, attention_mask=full_mask).logits  # [B, P+R, V]
            old_logp_tokens = F.log_softmax(old_logits[:, :-1], dim=-1).gather(2, labels.unsqueeze(-1)).squeeze(-1)  # [B, P+R-1]
            old_logp = (old_logp_tokens * final_mask).sum(dim=1)  # [B]
            
            ref_logits = ref_model(input_ids=gen_out, attention_mask=full_mask).logits  # [B, P+R, V]
            ref_logp_tokens = F.log_softmax(ref_logits[:, :-1], dim=-1).gather(2, labels.unsqueeze(-1)).squeeze(-1)  # [B, P+R-1]
            ref_logp = (ref_logp_tokens * final_mask).sum(dim=1)  # [B]

        '''Step 5: è®¡ç®—æŸå¤±å‡½æ•° (Loss Function)
            PPOçš„æ€»Lossä¸€èˆ¬ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼š
            1. ç­–ç•¥æŸå¤± (Policy Loss)ï¼šè®©ä¼˜åŠ¿ï¼ˆAdvantageï¼‰é«˜çš„åŠ¨ä½œæ¦‚ç‡å˜å¤§ã€‚
            2. ä»·å€¼å‡½æ•°æŸå¤± (Value Function Loss)ï¼šè®© Critic é¢„æµ‹å¾—æ›´å‡†ã€‚
            3. KLæ•£åº¦æƒ©ç½š (KL Divergence Penalty)ï¼šå¼ºè¿« Actor ä¸è¦èƒŒç¦» Reference Model (SFTæ¨¡å‹) å¤ªè¿œï¼Œé˜²æ­¢å®ƒä¸ºäº†å–æ‚¦ Reward Model è€Œè¾“å‡ºä¹±ç ï¼ˆReward Hackingï¼‰ã€‚
            å…¬å¼ï¼š
            L = -E[min(r_t * A_t, clip(r_t, 1-Îµ, 1+Îµ) * A_t)] + C1 * V_loss + C2 * KL_loss
        '''
        # 1. ç­–ç•¥æŸå¤±
        ratio = torch.exp(actor_logp - old_logp)  # [B]
        surr1 = ratio * advantages  # [B]
        surr2 = torch.clamp(ratio, 1.0 - args.clip_epsilon, 1.0 + args.clip_epsilon) * advantages  # [B]
        policy_loss = -torch.min(surr1, surr2).mean()  # scalar
        # 2. ä»·å€¼å‡½æ•°æŸå¤±
        value_loss = F.mse_loss(values, rewards)  # scalar
        # 3. KLæ•£åº¦æƒ©ç½šé¡¹
        kl_ref = (actor_logp - ref_logp).mean()
        kl = (actor_logp - old_logp).mean()  # ç”¨äºç›‘æ§ï¼šå½“å‰ç­–ç•¥ç›¸å¯¹äºä¸Šä¸€æ­¥ç­–ç•¥çš„å˜åŒ–å¹…åº¦

        loss = policy_loss + args.vf_coef * value_loss + args.kl_coef * kl_ref  # scalar
        loss.backward()


        '''Step 6: æ¢¯åº¦æ›´æ–°ä¸æ—§ç­–ç•¥æ›´æ–°'''
        if (step + 1) % args.accumulation_steps == 0:
            # æ¢¯åº¦è£å‰ªåœ¨ RLHF ä¸­éå¸¸é‡è¦ï¼Œå› ä¸ºå¼ºåŒ–å­¦ä¹ çš„æ¢¯åº¦æ–¹å·®å¾ˆå¤§ï¼Œå®¹æ˜“å¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚
            clip_grad_norm_(actor_model.parameters(), args.grad_clip)  # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            clip_grad_norm_(critic_model.parameters(), args.grad_clip)
            actor_optimizer.step()   # æ›´æ–° Actor å‚æ•°
            critic_optimizer.step()  # æ›´æ–° Critic å‚æ•°
            actor_scheduler.step()   # æ›´æ–°å­¦ä¹ ç‡
            critic_scheduler.step()
            actor_optimizer.zero_grad() # æ¸…ç©ºæ¢¯åº¦
            critic_optimizer.zero_grad()
            torch.cuda.empty_cache() # ç¨å¾®æ¸…ç†æ˜¾å­˜

        if is_main_process():
            # --- è®¡ç®—ç”Ÿæˆçš„å¹³å‡é•¿åº¦ ---
            response_ids = gen_out[:, enc.input_ids.shape[1]:] # åˆ‡åˆ†å‡º Response éƒ¨åˆ†
            is_eos = (response_ids == tokenizer.eos_token_id)  # æ‰¾åˆ° EOS token
            eos_indices = torch.argmax(is_eos.int(), dim=1)    # æ‰¾åˆ°æ¯è¡Œç¬¬ä¸€ä¸ª EOS çš„ä½ç½®
            has_eos = is_eos.any(dim=1)                        # åˆ¤æ–­æ˜¯å¦æœ‰ EOS
            # å¦‚æœæœ‰ EOSï¼Œé•¿åº¦å°±æ˜¯ EOS çš„ç´¢å¼•+1ï¼›å¦‚æœæ²¡æœ‰ï¼Œé•¿åº¦å°±æ˜¯æœ€å¤§ç”Ÿæˆé•¿åº¦
            lengths = torch.where(has_eos, eos_indices + 1, torch.tensor(response_ids.shape[1], device=is_eos.device))
            avg_len = lengths.float().mean()

            actor_loss_val = policy_loss.item()
            critic_loss_val = value_loss.item()
            reward_val = rewards.mean().item()
            kl_val = kl.item()
            kl_ref_val = kl_ref.item()
            avg_len_val = avg_len.item()
            actor_lr = actor_optimizer.param_groups[0]['lr']
            critic_lr = critic_optimizer.param_groups[0]['lr']

            if wandb is not None:
                wandb.log({
                    "actor_loss": actor_loss_val,   # é€šå¸¸ä¼šéœ‡è¡ï¼Œä¸å¦‚ Reward ç›´è§‚ã€‚
                    "critic_loss": critic_loss_val, # é€šå¸¸ä¼šéœ‡è¡ï¼Œä¸å¦‚ Reward ç›´è§‚ã€‚
                    "reward": reward_val,   # æœ€é‡è¦çš„æŒ‡æ ‡ï¼Œåº”è¯¥å‘ˆä¸Šå‡è¶‹åŠ¿ã€‚
                    "kl": kl_val,
                    "kl_ref": kl_ref_val,   # åº”è¯¥ç»´æŒåœ¨ä¸€ä¸ªè¾ƒä½æ°´å¹³ï¼Œå¦‚æœé£™å‡è¯´æ˜æ¨¡å‹å´©äº†ï¼ˆMode Collapseï¼‰ã€‚
                    "avg_response_len": avg_len_val,
                    "actor_lr": actor_lr,
                })
            #
            Logger(f"Epoch: {epoch+1}, Step: {step}/{iters}, "
                   f"Actor Loss: {actor_loss_val:.6f}, Critic Loss: {critic_loss_val:.6f}, "
                   f"Reward: {reward_val:.6f}, KL: {kl_val:.6f}, KL_ref: {kl_ref_val:.6f}, "
                   f"Avg Response Len: {avg_len_val:.2f}, Actor LR: {actor_lr:.2e}, Critic LR: {critic_lr:.2e}")

        if (step + 1) % args.update_old_actor_freq == 0:
            '''
            ç›®çš„: PPO è¦æ±‚ ratio ä¸­çš„åˆ†æ¯ Ï€_old æ˜¯â€œé‡‡æ ·æ—¶çš„ç­–ç•¥â€ã€‚ä½†ä¸ºäº†èŠ‚çœæ˜¾å­˜å’Œå·¥ç¨‹æ–¹ä¾¿ï¼Œè¿™é‡Œé‡‡ç”¨ Rolling Update ç­–ç•¥ã€‚
            æ¯éš” update_old_actor_freq æ­¥ï¼Œå°±æŠŠå½“å‰çš„ actor_model å¤åˆ¶ä¸€ä»½ç»™ old_actor_modelã€‚
            è¿™æ ·ä¿è¯äº† old_actor å§‹ç»ˆç´§è·Ÿ actorï¼Œä½¿å¾— ratio æ¥è¿‘ 1ï¼Œæ»¡è¶³ PPO çš„è¿‘ä¼¼æ¡ä»¶ã€‚
            å·¥ç¨‹ç»†èŠ‚: å…ˆè½¬åˆ° CPU å†è½¬å› GPU æˆ–è€…æ˜¯ä¸ºäº†é˜²æ­¢æ˜¾å­˜ç¢ç‰‡åŒ–ï¼Œæˆ–è€…è§„é¿æŸäº› DDP çš„æ­»é”é£é™©ï¼ˆè§†å…·ä½“ç¯å¢ƒè€Œå®šï¼‰ã€‚
            '''
            state_dict = actor_model.module.state_dict() if isinstance(actor_model, DistributedDataParallel) else actor_model.state_dict()
            old_actor_model.load_state_dict({k: v.detach().cpu() for k, v in state_dict.items()})
            old_actor_model.to(args.device)

        if (step % args.save_interval == 0 or step == iters - 1) and is_main_process():
            actor_model.eval()
            moe_suffix = '_moe' if lm_config.use_moe else ''
            ckp = f'{args.save_dir}/{args.save_weight}_{lm_config.hidden_size}{moe_suffix}.pth'
            actor_state = actor_model.module.state_dict() if isinstance(actor_model, DistributedDataParallel) else actor_model.state_dict()
            # ä¿å­˜è½»é‡çº§æƒé‡ (BFloat16/Float16)
            torch.save({k: v.half().cpu() for k, v in actor_state.items()}, ckp)
            
            # ä½¿ç”¨ lm_checkpoint ä¿å­˜å®Œæ•´çŠ¶æ€ï¼ˆåŒ…æ‹¬ criticã€ä¼˜åŒ–å™¨çŠ¶æ€ç­‰ï¼Œç”¨äºæ–­ç‚¹ç»­è®­)
            lm_checkpoint(lm_config, weight=args.save_weight, model=actor_model, optimizer=actor_optimizer, 
                         epoch=epoch, step=step, wandb=wandb, save_dir='../checkpoints',
                         scheduler=actor_scheduler, critic_model=critic_model, 
                         critic_optimizer=critic_optimizer, critic_scheduler=critic_scheduler)
            actor_model.train()
            del actor_state

        # --- æ¿€è¿›çš„æ˜¾å­˜æ¸…ç† 
        # ---è¿™é‡Œæ˜¾å¼åœ° del æ‰æ‰€æœ‰ä¸­é—´å˜é‡ï¼ˆç‰¹åˆ«æ˜¯è®¡ç®—å›¾ç›¸å…³çš„ Tensorï¼‰ï¼Œå¹¶å¼ºåˆ¶æ¸…ç©º CUDA ç¼“å­˜ï¼Œæ˜¯ä¸ºäº†é˜²æ­¢ OOM (Out of Memory)ï¼Œç¡®ä¿ä¸‹ä¸€ä¸ª Batch èƒ½é¡ºåˆ©è·‘èµ·æ¥ã€‚
        del enc, gen_out, responses_text, rewards, full_mask, values_seq, values, advantages
        del logits, labels, logp_tokens, final_mask, actor_logp, old_logits, old_logp, ref_logits, ref_logp
        del kl, kl_ref, ratio, surr1, surr2, policy_loss, value_loss, loss
        torch.cuda.empty_cache()

if __name__ == "__main__":
    torch.autograd.set_detect_anomaly(True)

    parser = argparse.ArgumentParser(description="MiniMind PPO (Proximal Policy Optimization)")
    parser.add_argument("--save_dir", type=str, default="../out", help="æ¨¡å‹ä¿å­˜ç›®å½•")
    parser.add_argument('--save_weight', default='ppo_actor', type=str, help="ä¿å­˜æƒé‡çš„å‰ç¼€å")
    parser.add_argument("--epochs", type=int, default=1, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=2, help="batch size")
    parser.add_argument("--learning_rate", type=float, default=8e-8, help="Actorå­¦ä¹ ç‡")
    parser.add_argument("--critic_learning_rate", type=float, default=8e-8, help="Criticå­¦ä¹ ç‡")
    parser.add_argument("--device", type=str, default="cuda:0" if torch.cuda.is_available() else "cpu", help="è®­ç»ƒè®¾å¤‡")
    parser.add_argument("--dtype", type=str, default="bfloat16", help="æ··åˆç²¾åº¦ç±»å‹")
    parser.add_argument("--num_workers", type=int, default=1, help="æ•°æ®åŠ è½½çº¿ç¨‹æ•°")
    parser.add_argument("--accumulation_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
    parser.add_argument("--log_interval", type=int, default=1, help="æ—¥å¿—æ‰“å°é—´éš”")
    parser.add_argument("--save_interval", type=int, default=10, help="æ¨¡å‹ä¿å­˜é—´éš”")
    parser.add_argument('--hidden_size', default=512, type=int, help="éšè—å±‚ç»´åº¦")
    parser.add_argument('--num_hidden_layers', default=8, type=int, help="éšè—å±‚æ•°é‡")
    parser.add_argument('--use_moe', default=0, type=int, choices=[0, 1], help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")
    parser.add_argument('--max_seq_len', default=66, type=int, help="Promptæœ€å¤§é•¿åº¦")
    parser.add_argument("--max_gen_len", type=int, default=1536, help="ç”Ÿæˆçš„æœ€å¤§é•¿åº¦")
    parser.add_argument("--data_path", type=str, default="../dataset/rlaif-mini.jsonl", help="RLAIFæ•°æ®è·¯å¾„")
    parser.add_argument("--clip_epsilon", type=float, default=0.1, help="PPOè£å‰ªå‚æ•°")
    parser.add_argument("--vf_coef", type=float, default=0.5, help="Value functionç³»æ•°")
    parser.add_argument("--kl_coef", type=float, default=0.02, help="KLæ•£åº¦æƒ©ç½šç³»æ•°")
    parser.add_argument("--reasoning", type=int, default=1, choices=[0, 1], help='æ¨ç†æ¨¡å‹ç±»å‹ï¼ˆ0=æ™®é€šæ¨¡å‹ï¼Œ1=æ¨ç†æ¨¡å‹ï¼‰')
    parser.add_argument("--update_old_actor_freq", type=int, default=4, help="æ›´æ–°old_actor_modelçš„é¢‘ç‡")
    parser.add_argument("--reward_model_path", type=str, default="~/models/internlm2-1_8b-reward", help="Rewardæ¨¡å‹è·¯å¾„")
    parser.add_argument('--from_resume', default=0, type=int, choices=[0, 1], help="æ˜¯å¦è‡ªåŠ¨æ£€æµ‹&ç»­è®­ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")
    parser.add_argument("--use_wandb", action="store_true", help="æ˜¯å¦ä½¿ç”¨wandb")
    parser.add_argument("--wandb_project", type=str, default="MiniMind-PPO", help="wandbé¡¹ç›®å")
    args = parser.parse_args()

    # ========== 1. åˆå§‹åŒ–ç¯å¢ƒå’Œéšæœºç§å­ ==========
    local_rank = init_distributed_mode()
    if dist.is_initialized(): args.device = f"cuda:{local_rank}"
    setup_seed(42 + (dist.get_rank() if dist.is_initialized() else 0))
    
    # ========== 2. é…ç½®ç›®å½•ã€æ¨¡å‹å‚æ•°ã€æ£€æŸ¥ckp ==========
    os.makedirs(args.save_dir, exist_ok=True)
    lm_config = MiniMindConfig(hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers, use_moe=bool(args.use_moe))
    ckp_data = lm_checkpoint(lm_config, weight=args.save_weight, save_dir='../checkpoints') if args.from_resume==1 else None
    
    # ========== 3. è®¾ç½®æ··åˆç²¾åº¦ ==========
    device_type = "cuda" if "cuda" in args.device else "cpu"
    # dtype = torch.bfloat16 if args.dtype == "bfloat16" else torch.float16
    # autocast_ctx = nullcontext() if device_type == "cpu" else torch.cuda.amp.autocast(dtype=dtype)

    # ä¿®æ­£åçš„é€»è¾‘ï¼šæ­£ç¡®è¯†åˆ« float32
    if args.dtype == "float32":
        dtype = torch.float32
    elif args.dtype == "bfloat16":
        dtype = torch.bfloat16
    else:
        dtype = torch.float16

    # å…³é”®ä¿®æ”¹ï¼šå¦‚æœæ˜¯ float32ï¼Œå¿…é¡»ç¦ç”¨ autocastï¼
    # å¦åˆ™ autocast å¯èƒ½ä¼šåœ¨åå°æé¬¼
    if dtype == torch.float32:
        autocast_ctx = nullcontext()
        print("DEBUG: Autocast DISABLED for float32 training.")
    else:
        autocast_ctx = torch.cuda.amp.autocast(dtype=dtype)
        print(f"DEBUG: Autocast ENABLED with {dtype}.")
    
    
    # ========== 4. é…wandb ==========
    wandb = None
    if args.use_wandb and is_main_process():
        import swanlab as wandb
        wandb_id = ckp_data.get('wandb_id') if ckp_data else None
        resume = 'must' if wandb_id else None
        wandb_run_name = f"MiniMind-PPO-Epoch-{args.epochs}-BS-{args.batch_size}-LR-{args.learning_rate}"
        wandb.init(project=args.wandb_project, name=wandb_run_name, id=wandb_id, resume=resume)

    # ========== 5. åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ® ==========
    base_weight = "reason" if args.reasoning == 1 else "full_sft"
    # Actoræ¨¡å‹
    actor_model, tokenizer = init_model(lm_config, base_weight, device=args.device)
    tokenizer.padding_side = 'left'  # PPOéœ€è¦å·¦ä¾§padding
    # Old Actoræ¨¡å‹
    old_actor_model, _ = init_model(lm_config, base_weight, device=args.device)
    old_actor_model = old_actor_model.eval().requires_grad_(False)
    # Referenceæ¨¡å‹
    ref_model, _ = init_model(lm_config, base_weight, device=args.device)
    ref_model = ref_model.eval().requires_grad_(False)
    # Criticæ¨¡å‹
    moe_suffix = '_moe' if lm_config.use_moe else ''
    ckp = f'{args.save_dir}/{base_weight}_{lm_config.hidden_size}{moe_suffix}.pth'
    state_dict = torch.load(ckp, map_location=args.device)
    critic_model = CriticModel(lm_config)
    critic_model.load_state_dict(state_dict, strict=False)
    critic_model = critic_model.to(args.device)
    # Rewardæ¨¡å‹
    reward_model = AutoModel.from_pretrained(
        args.reward_model_path, torch_dtype=torch.float16, trust_remote_code=True
    )
    reward_model = reward_model.to(args.device).eval().requires_grad_(False)
    reward_tokenizer = AutoTokenizer.from_pretrained(args.reward_model_path, trust_remote_code=True)
    # æ•°æ®å’Œä¼˜åŒ–å™¨
    train_ds = RLAIFDataset(args.data_path, tokenizer, max_length=(args.max_seq_len + args.max_gen_len))
    train_sampler = DistributedSampler(train_ds) if dist.is_initialized() else None
    actor_optimizer = optim.AdamW(actor_model.parameters(), lr=args.learning_rate)
    critic_optimizer = optim.AdamW(critic_model.parameters(), lr=args.critic_learning_rate)
    loader_for_count = DataLoader(train_ds, batch_size=args.batch_size, sampler=train_sampler)
    iters = len(loader_for_count)
    total_optimizer_steps = (iters // args.accumulation_steps) * args.epochs
    actor_scheduler = CosineAnnealingLR(actor_optimizer, T_max=total_optimizer_steps, eta_min=args.learning_rate / 10)
    critic_scheduler = CosineAnnealingLR(critic_optimizer, T_max=total_optimizer_steps, eta_min=args.critic_learning_rate / 10)

    print(f"DEBUG CHECK [Start]")
    print(f"Tokenizer vocab size (len): {len(tokenizer)}")
    print(f"Model vocab size (config): {lm_config.vocab_size}")
    
    # æ£€æŸ¥ Embedding å±‚çš„å®é™…å¤§å°
    if hasattr(actor_model, 'module'):
        # å¦‚æœæ˜¯DDPï¼Œå–module
        embed_weight = actor_model.module.model.embed_tokens.weight
    else:
        embed_weight = actor_model.model.embed_tokens.weight
        
    print(f"Model Embedding weight shape: {embed_weight.shape}")
    
    if len(tokenizer) > lm_config.vocab_size:
        print("\n[CRITICAL ERROR DETECTED!]")
        print(f"Tokenizer åŒ…å« {len(tokenizer)} ä¸ª tokenï¼Œä½†æ¨¡å‹åªå®šä¹‰äº† {lm_config.vocab_size} ä¸ªæ§½ä½ã€‚")
        print("è¿™ä¼šå¯¼è‡´ Embedding å±‚è¶Šç•Œå´©æºƒï¼")
        exit(1)
    
    print(f"DEBUG CHECK [End]")

    # ========== 6. ä»ckpæ¢å¤çŠ¶æ€ ==========
    start_epoch, start_step = 0, 0
    if ckp_data:
        actor_model.load_state_dict(ckp_data['model'])
        critic_model.load_state_dict(ckp_data['critic_model'])
        actor_optimizer.load_state_dict(ckp_data['optimizer'])
        critic_optimizer.load_state_dict(ckp_data['critic_optimizer'])
        actor_scheduler.load_state_dict(ckp_data['scheduler'])
        critic_scheduler.load_state_dict(ckp_data['critic_scheduler'])
        start_epoch = ckp_data['epoch']
        start_step = ckp_data.get('step', 0)
    
    # ========== 7. DDPåŒ…æ¨¡å‹ ==========
    if dist.is_initialized():
        actor_model._ddp_params_and_buffers_to_ignore = {"freqs_cos", "freqs_sin"}
        critic_model._ddp_params_and_buffers_to_ignore = {"freqs_cos", "freqs_sin"}
        actor_model = DistributedDataParallel(actor_model, device_ids=[local_rank])
        critic_model = DistributedDataParallel(critic_model, device_ids=[local_rank])
        old_actor_model.to(args.device)
    
    # ========== 8. å¼€å§‹è®­ç»ƒ ==========
# æ¨¡å‹æƒé‡æ£€æŸ¥===============================
    print("-" * 30)
    print("æ­£åœ¨æ£€æŸ¥æ¨¡å‹æƒé‡å¥åº·çŠ¶å†µ...")
    has_nan = False
    for name, param in actor_model.named_parameters():
        if torch.isnan(param).any() or torch.isinf(param).any():
            print(f"ğŸ”´ CRITICAL WARNING: Parameter [{name}] contains NaN or Inf!")
            print(f"   - Max: {param.max()}, Min: {param.min()}")
            has_nan = True
            
    if has_nan:
        print("âŒ æ¨¡å‹æƒé‡æ–‡ä»¶å·²æŸåï¼ˆåŒ…å«NaNï¼‰ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒã€‚")
        print("è¯·æ£€æŸ¥ `args.save_dir` æˆ– `ckp` æŒ‡å‘çš„è·¯å¾„ï¼Œåˆ é™¤åçš„æƒé‡æ–‡ä»¶ï¼Œé‡æ–°å¼€å§‹ã€‚")
        exit(1)
    else:
        print("âœ… æ¨¡å‹æƒé‡æ£€æŸ¥é€šè¿‡ï¼Œæ•°å€¼æ­£å¸¸ã€‚")
    print("-" * 30)
# ==========================================


# ==========================================
# ğŸ•µï¸â€â™‚ï¸ NaN ä¾¦æ¢ï¼šæ³¨å†Œ Hook ç²¾å‡†å®šä½æ•…éšœå±‚
# ==========================================
    print("\nğŸ•µï¸â€â™‚ï¸ æ­£åœ¨æ³¨å†Œ NaN ç›‘æ§é’©å­ (Layer Hooks)...")
    
    def detect_nan_hook(module, input, output):
        # 1. æå– Output Tensor
        if isinstance(output, tuple):
            tensor_out = output[0]
        else:
            tensor_out = output
        
        # 2. æ£€æŸ¥ NaN æˆ– Inf
        if isinstance(tensor_out, torch.Tensor):
            if torch.isnan(tensor_out).any() or torch.isinf(tensor_out).any():
                print(f"\nğŸ”´ [CRITICAL ERROR] NaN/Inf DETECTED!")
                print(f"ğŸ“ Layer Type: {module.__class__.__name__}")
                print(f"ğŸ“ Layer Name: {module}")
                
                # æ£€æŸ¥è¾“å…¥æƒ…å†µ
                if len(input) > 0 and isinstance(input[0], torch.Tensor):
                    print(f"   Input Stat: min={input[0].min().item():.4f}, max={input[0].max().item():.4f}, mean={input[0].mean().item():.4f}")
                    if torch.isnan(input[0]).any():
                        print("   (è¾“å…¥æœ¬èº«å°±å·²ç»åŒ…å« NaN äº†ï¼Œè¯´æ˜æ˜¯ä¸Šä¸€å±‚ä¼ ä¸‹æ¥çš„)")
                
                # æ£€æŸ¥è¾“å‡ºæƒ…å†µ
                print(f"   Output Stat: {tensor_out}")
                print("ğŸ›‘ åœæ­¢è¿è¡Œï¼Œè¯·åˆ†æä¸Šè¿°æŠ¥é”™å±‚ã€‚")
                import sys; sys.exit(1)

    # è·å–å®é™…æ¨¡å‹ (è§£åŒ… DDP)
    real_model = actor_model.module if hasattr(actor_model, "module") else actor_model
    
    # ä¸ºæ¯ä¸€å±‚æ³¨å†Œ Hook
    for name, submodule in real_model.named_modules():
        submodule.register_forward_hook(detect_nan_hook)
        
    print("âœ… Hook æ³¨å†Œå®Œæˆï¼Œå‡†å¤‡æ•æ‰ NaN...\n")
# ==========================================

    for epoch in range(start_epoch, args.epochs):
        train_sampler and train_sampler.set_epoch(epoch)
        if epoch == start_epoch and start_step > 0:  # ç¬¬ä¸€ä¸ªepochä¸”å­˜åœ¨æ£€æŸ¥ç‚¹
            batch_sampler = SkipBatchSampler(train_sampler or range(len(train_ds)), args.batch_size, start_step + 1)
            loader = DataLoader(train_ds, batch_sampler=batch_sampler, num_workers=args.num_workers, pin_memory=True)
            Logger(f'Epoch [{epoch + 1}/{args.epochs}]: è·³è¿‡å‰{start_step}ä¸ªstepï¼Œä»step {start_step + 1}å¼€å§‹')
            ppo_train_epoch(epoch, loader, len(loader) + start_step + 1, old_actor_model, ref_model, 
                           actor_scheduler, critic_scheduler, reward_model, reward_tokenizer, start_step, wandb)
        else:  # é»˜è®¤ä»å¤´å¼€å§‹
            loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=(train_sampler is None), 
                              sampler=train_sampler, num_workers=args.num_workers, pin_memory=True)
            ppo_train_epoch(epoch, loader, len(loader), old_actor_model, ref_model, 
                           actor_scheduler, critic_scheduler, reward_model, reward_tokenizer, 0, wandb)