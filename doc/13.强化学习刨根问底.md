# PPO
### PPO损失函数

$$
\arg\max_g \mathbb{E}_{s\sim\nu', a\sim\pi_{\theta_t}(\cdot|s)} \left[ \min \left( \frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_t}(a \mid s)} A^{\pi_{\theta_t}}(s, a), clip\left( \frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_t}(a \mid s)}, 1 - \epsilon, 1 + \epsilon \right) A^{\pi_{\theta_t}}(s, a) \right) \right]
$$

### GAE 估计部分

$$
\begin{aligned}
A_t^{(1)} &= \delta_t = -V(s_t) + r_t + V(s_{t+1}) \\
A_t^{(2)} &= \delta_t + \gamma \delta_{t+1} = -V(s_t) + r_t + \gamma r_{t+1} + \gamma^2 V(s_{t+2}) \\
A_t^{(3)} &= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} = -V(s_t) + r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 V(s_{t+3}) \\
&\vdots \\
A_t^{(k)} &= \sum_{l=0}^{k-1} \gamma^l \delta_{t+l} = -V(s_t) + r_t + \gamma r_{t+1} + \cdots + \gamma^{k-1} r_{t+k-1} + \gamma^k V(s_{t+k}) \\
\hat{A}_t^{\text{GAE}} &= (1 - \lambda)(\hat{A}_t^{(1)} + \lambda \hat{A}_t^{(2)} + \lambda^2 \hat{A}_t^{(3)} + \cdots) \\
&= (1 - \lambda) \left( \delta_t + \lambda (\delta_t + \gamma \delta_{t+1}) + \lambda^2 (\delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2}) + \cdots \right) \\
&= (1 - \lambda) \left( \delta_t (1 + \lambda + \lambda^2 + \cdots) + \gamma \delta_{t+1} (\lambda + \lambda^2 + \lambda^3 + \cdots) + \gamma^2 \delta_{t+2} (\lambda^2 + \lambda^3 + \cdots) + \cdots \right) \\
&= (1 - \lambda) \left( \frac{\delta_t}{1 - \lambda} + \frac{\gamma \delta_{t+1}}{1 - \lambda} + \frac{\gamma^2 \delta_{t+2}}{1 - \lambda} + \cdots \right) \\
&= \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
\end{aligned}
$$

## 代码实现
### GAE

```python
def compute_advantage(gamma, lmbda, td_delta):
	td_delta = td_delta.detach().numpy()
	advantage_list = []
	advantage = 0.0
	for delta in td_delta[::-1]:
		advantage = gamma * lmbda * advantage + delta
		advantage_list.append(advantage)
	advantage_list.reverse()
	return torch.tensor(advantage_list, dtype=torch.float)
```
这段代码实现了 **Generalized Advantage Estimation (GAE)** 的核心计算逻辑。GAE 是一种在强化学习（如 PPO、TRPO 算法）中常用的优势函数（Advantage Function）估计方法，旨在平衡方差（Variance）和偏差（Bias）。

下面结合 **数学公式** 和 **代码逻辑** 逐步解释。

#### 1. 核心数学公式

首先定义 **TD Error（时序差分误差）** $\delta_t$。这也是代码的输入参数 `td_delta`：
$$ \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $$

GAE 的定义是一个加权几何平均，其展开形式为：
$$ \hat{A}_t^{GAE(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l} $$
即：
$$ \hat{A}_t = \delta_t + (\gamma \lambda)\delta_{t+1} + (\gamma \lambda)^2\delta_{t+2} + \dots $$

为了在代码中高效计算，我们通常使用 **递归形式（Recursive Form）**。观察上面的展开式，可以发现：
$$ \hat{A}_{t+1} = \delta_{t+1} + (\gamma \lambda)\delta_{t+2} + \dots $$

因此，$A_t$ 可以写成当前时刻的 TD error 加上下一时刻优势函数的衰减值：
$$ \hat{A}_t = \delta_t + (\gamma \lambda) \hat{A}_{t+1} $$

**这就是代码第 6 行所使用的公式。**

---

#### 2. 代码逐行解释

我们将代码逻辑映射到上述递归公式中。

##### 参数含义
*   `gamma` ($\gamma$): 折扣因子（Discount Factor）。
*   `lmbda` ($\lambda$): GAE 平滑因子，用于控制偏差-方差权衡。
*   `td_delta`: 一个包含 $[\delta_0, \delta_1, \dots, \delta_T]$ 的序列。

##### 逻辑流程

**第 5 行：倒序遍历**
```python
for delta in td_delta[::-1]:
```
*   **解释**：代码使用了 `[::-1]` 对 `td_delta` 进行倒序切片。
*   **原因**：根据递归公式 $\hat{A}_t = \delta_t + (\gamma \lambda) \hat{A}_{t+1}$，计算当前时刻 $t$ 的优势值需要知道下一时刻 $t+1$ 的优势值。因此，必须**从后往前**（从时间步 $T$ 到 $0$）计算。

**第 6 行：递归计算**
```python
advantage = gamma * lmbda * advantage + delta
```
*   **对应公式**：
    $$ \underbrace{\hat{A}_t}_{\text{新 advantage}} = \underbrace{\delta_t}_{\text{delta}} + \underbrace{(\gamma \lambda)}_{\text{gamma * lmbda}} \times \underbrace{\hat{A}_{t+1}}_{\text{旧 advantage}} $$
*   **细节**：
    *   在循环开始前，`advantage` 被初始化为 `0.0`（第 4 行），这代表最终状态之后（$T+1$）的优势值为 0。
    *   在倒序循环中，等号右边的 `advantage` 实际上是上一次循环计算出的结果（即时间步 $t+1$ 的值），加上当前的 `delta` 后，赋值给左边，更新为时间步 $t$ 的值。

**第 7-8 行：存储与恢复顺序**
```python
advantage_list.append(advantage)
...
advantage_list.reverse()
```
*   **解释**：
    *   由于是倒序遍历，`advantage_list` 中存入的顺序是 $[\hat{A}_T, \hat{A}_{T-1}, \dots, \hat{A}_0]$。
    *   为了与原始数据（状态、动作）的时间顺序对应，必须使用 `reverse()` 将其翻转回 $[\hat{A}_0, \hat{A}_1, \dots, \hat{A}_T]$。

#### 总结

这段代码通过**反向迭代**，利用 GAE 的**递归性质**高效地（$O(T)$ 复杂度）计算了整个轨迹的优势函数。

*   当 `lmbda = 0` 时，$\hat{A}_t = \delta_t$（即普通的 TD Error，偏差大，方差小）。
*   当 `lmbda = 1` 时，$\hat{A}_t = \sum \gamma^l r_{t+l} - V(s_t)$（即 Monte Carlo 估计，无偏差，方差大）。
*   GAE 通过 $\lambda$ 在两者之间取得平衡。

### Policy Loss
```python
class PolicyLoss(nn.Module):
	"""
	Policy Loss for PPO
	"""
	def __init__(self, clip_eps: float = 0.2) -> None:
		super().__init__()
		self.clip_eps = clip_eps
		
	def forward(
		self, 
		log_probs: torch.Tensor,
		old_log_probs: torch.Tensor,
		advantages: torch.Tensor,
		action_mask: Optional[torch.Tensor] = None,
	) -> torch.Tensor:
		ratio = (log_probs - old_log_probs).exp()
		surr1 = ratio * advantages
		surr2 = ratio.clamp(1 - self.clip_eps, 1 + self.clip_eps) * advantages
		loss = -torch.min(surr1, surr2) # [B, seq_len]
		loss = masked_mean(loss, action_mask, dim=-1).mean() # [B, seq_len] -> [B, ] -> Scalar (标量，0维张量)
		return loss
	
	def masked_mean(tensor, mask, dim):
		if mask is None:
			return tensor.mean(axis=dim)
		return (tensor * mask).sum(axis=dim) / mask.sum(axis=dim)
```

## Value Loss
$$
 L^{VF}_{CLIP}(\theta) = \frac{1}{2} \max \left[ (V_\theta(s_t) - R_t)^2, \ (V_{\theta}^{\text{clip}}(s_t) - R_t)^2 \right] 
$$

```python
class ValueLoss(nn.Module):
	"""
	values: 实时critic得到的预估预期收益
	old_values: 更新前critic得到的预估预期收益
	returns：实际预期收益
	mask： response部分的mask
	"""
	def __init__(self, clip_eps: float = None) -> None:
		super().__init__()
		self.clip_eps = clip_eps
		
	def forward(
		self, 
		values: torch.Tensor,
		old_values: torch.Tensor,
		returns: torch.Tensor,
		action_mask: Optional[torch.Tensor] = None,
	) -> torch.Tensor:
		# 这里对Value也进行了clip操作，道理和actor loss的clip一致
		if self.clip_eps is not None:
			values_clipped = old_values + (values - old_values).clamp(self.clip_eps, self.clip_eps)
			surr1 = (values - returns) ** 2
			surr2 = (values_clipped - returns) ** 2
			loss = torch.max(surr1, surr2)
		else:
			loss = (values - returns) ** 2
		loss = masked_mean(loss, action_mask, dim=-1).mean()
		return 0.5 * loss
```

这段代码实现了 **PPO (Proximal Policy Optimization)** 算法中的 **Critic (Value Function) Loss**。

它的核心目的是训练 Critic 网络（价值网络），使其预测的状态价值 $V(s)$ 尽可能接近真实的收益回报 $R$（Returns），同时像 Actor Loss 一样引入了 **Clipping（截断）机制** 来防止网络参数更新幅度过大。

---

### 1. 数学公式对应

Critic 的目标是最小化预测价值与真实回报之间的 **均方误差 (MSE)**。

#### 标准 MSE Loss (无 Clip)
当 `clip_eps` 为 `None` 或代码走 `else` 分支时（第 24-25 行）：
$$ L^{VF}(\theta) = \frac{1}{2} (V_\theta(s_t) - R_t)^2 $$

#### PPO Clipped Value Loss (有 Clip)
当启用 `clip_eps` 时（第 19-23 行），为了保证 Critic 更新的稳定性，使用了如下公式：

$$ L^{VF}_{CLIP}(\theta) = \frac{1}{2} \max \left[ (V_\theta(s_t) - R_t)^2, \ (V_{\theta}^{\text{clip}}(s_t) - R_t)^2 \right] $$

其中截断价值 $V_{\theta}^{\text{clip}}$ 定义为：
$$ V_{\theta}^{\text{clip}}(s_t) = V_{\theta_{old}}(s_t) + \text{clip}(V_\theta(s_t) - V_{\theta_{old}}(s_t), -\epsilon, \epsilon) $$

---

### 2. 代码逐行解析

#### A. 截断操作 (Line 20)
```python
values_clipped = old_values + (values - old_values).clamp(-self.clip_eps, self.clip_eps)
```
*   **公式**：$V_{\theta}^{\text{clip}} = V_{old} + \text{clip}(V_{new} - V_{old}, -\epsilon, \epsilon)$
*   **含义**：它限制了新预测的 Value 相对于旧 Value 的变化幅度。无论网络想把价值预测得多么不同，`values_clipped` 强制将其限制在旧价值的 $\pm \epsilon$ 邻域内。

#### B. 计算两个 Surrogate Loss (Line 21-22)
```python
surr1 = (values_clipped - returns) ** 2
surr2 = (values - returns) ** 2
```
*   `surr2`：这是**原始的均方误差**，直接衡量当前网络预测值与目标的差距。
*   `surr1`：这是**截断后的均方误差**。如果网络预测值跑得太远（超出了 $\epsilon$ 范围），这里计算误差时用的是那个被强行拉回来的 `values_clipped`。

#### C. 取最大值 (Line 23)
```python
loss = torch.max(surr1, surr2)
```
*   **关键点**：为什么这里用 `max` 而 Policy Loss 用 `min`？
    *   在 **Policy Loss** 中，我们的目标函数是**最大化**奖励，取 `min` 是为了取一个“悲观下界”，防止过度乐观。
    *   在 **Value Loss** 中，我们的目标函数是**最小化**误差（Loss）。取 `max` 意味着我们选择**误差更大**的那一项作为 Loss。
*   **直观解释**：
    *   如果 $V_{new}$ 相比 $V_{old}$ 变化很大，且这种变化让预测更准了（`surr2` 很小），但 `surr1`（被拉回旧值后的误差）很大。
    *   取 `max` 会强迫 Loss 使用较大的 `surr1`。
    *   这实际上是在**惩罚** Value 网络的剧烈变化。即使你变动后预测得更准了，但只要你变动幅度太大（超过 $\epsilon$），我就给你一个很大的 Loss，强迫你“慢点更新”。

#### D. 系数 0.5 (Line 27)
```python
return 0.5 * loss
```
*   **数学习惯**：这对应公式前面的 $\frac{1}{2}$。
*   **原因**：在求导时，$(x^2)' = 2x$。乘上 $0.5$ 后，导数就变成了 $x$，计算梯度时更加整洁，没有额外的系数 $2$。

### 总结

这段代码实现了一个**带有“刹车机制”的均方误差损失**。

1.  它希望预测值 `values` 接近 `returns`。
2.  但如果 `values` 相比 `old_values` 跑得太快（超过 `clip_eps`），它会通过取 `max` 产生一个较大的 Loss，从而抑制这种剧烈的参数更新，保证训练的平稳性。

## RM Loss
```python
class PairWiseLoss(nn.Module):
	"""
	Pairwise Loss for Reword Model
	"""
	def __init__(self):
		super().__init__()
		
	def forward(self, chosen_reward, reject_reward, margin):
		if margin:
			loss = -F.logsigmoid(chosen_reward - reject_reward - margin)
		else :
			loss = -F.logsigmoid(chosen_reward - reject_reward)
		return loss.mean()
```
这段代码实现了 **RLHF (Reinforcement Learning from Human Feedback)** 流程中 **Reward Model (奖励模型)** 训练的核心损失函数：**Pairwise Ranking Loss**。

它的核心目标是：**训练奖励模型给“人类偏好（Chosen）”的回复打分比“拒绝（Rejected）”的回复更高。**

---

### 1. 核心数学公式 (Bradley-Terry Model)

Reward Model 的训练通常基于 **Bradley-Terry 模型**，该模型假设人类选择回复 $y_c$ (chosen) 优于 $y_r$ (rejected) 的概率为：

$$ P(y_c \succ y_r | x) = \sigma(r(x, y_c) - r(x, y_r)) $$

其中：
*   $r(x, y)$ 是奖励模型给出的分数。
*   $\sigma(z) = \frac{1}{1+e^{-z}}$ 是 Sigmoid 函数。

为了最大化这个概率（即最大似然估计 MLE），我们将问题转化为**最小化负对数似然 (Negative Log-Likelihood)**：

$$ L(\theta) = - \log \left( \sigma(r_{chosen} - r_{rejected}) \right) $$

这就是代码中 `else` 分支（无 Margin）对应的公式。

---

### 2. 代码逐行解析

#### **输入定义**
*   `chosen_reward` ($r_c$): 模型给“好回复”打的分数。
*   `reject_reward` ($r_r$): 模型给“坏回复”打的分数。
*   `margin` ($m$): 一个可选的间隔参数，用于增强模型的区分度。

#### **Line 6-7: 带 Margin 的情况**
```python
if margin is not None:
    loss = -F.logsigmoid(chosen_reward - reject_reward - margin)
```
*   **对应公式**：
    $$ L = - \log \sigma(r_c - r_r - m) $$
*   **解释**：
    *   这是标准 Ranking Loss 的一种变体（类似于 Hinge Loss 的思想）。
    *   它不仅要求 $r_c > r_r$，还希望 $r_c$ 至少比 $r_r$ 大 $m$ 个单位。
    *   如果在 Sigmoid 内部减去 $m$，那么为了让 Loss 变小（即让 Sigmoid 输出接近 1），$r_c - r_r$ 必须是一个比 $m$ 更大的正数。这迫使模型拉大好坏回复之间的分差。

#### **Line 8-9: 不带 Margin 的情况 (标准实现)**
```python
else:
    loss = -F.logsigmoid(chosen_reward - reject_reward)
```
*   **对应公式**：
    $$ L = - \log \sigma(r_c - r_r) $$
*   **逻辑推演**：
    1.  计算分差 $\Delta = r_c - r_r$。
    2.  如果 $\Delta$ 是很大的正数（模型判断正确），$\sigma(\Delta) \approx 1$，则 $\log(1) = 0$，Loss $\approx 0$。
    3.  如果 $\Delta$ 是负数（模型判断错误，坏的比好的分高），$\sigma(\Delta) \approx 0$，则 $\log(\text{small}) = \text{large negative}$，取负号后 Loss 变得很大。
*   **背景**：这是 **InstructGPT / ChatGPT** 论文中使用的标准 Reward Model 损失函数。

#### **Line 10: 平均化**
```python
return loss.mean()
```
*   **公式**：
    $$ J = \frac{1}{B} \sum_{i=1}^{B} L_i $$
*   **解释**：计算整个 Batch 中所有样本对的 Loss 平均值，用于反向传播。

---

### 3. 总结

这段代码通过 **`LogSigmoid`** 函数优雅地实现了 Ranking Loss：

1.  **输入**：一对分数（好回复 vs 坏回复）。
2.  **期望**：`chosen_reward` > `reject_reward`。
3.  **惩罚**：如果 `chosen` 不比 `reject` 高（或者高得不够多，在有 margin 的情况下），Loss 就会增大，引导模型调整参数以拉大两者的分差。
## Compute Reward
```python
def compute_reward(r, kl):
	kl_reward = -kl_coef * kl
	# 找到每个序列的结束位置(EOS)
	eos_indices = action_mask.size(1) - 1 - action_mask.long().fliplr().argmax(dim=1, keepdim=True)
	# 在结束的位置添加额外的奖励r
	last_reward = torch.zeros_like(kl).scatter_(dim=1, index=eos_indices, src=r.unsqueeze(1).to(kl.dtype))
	reward = last_reward + kl_reward
	return reward
```
这段代码实现了 **RLHF (Reinforcement Learning from Human Feedback)** 中最终 **Reward (奖励)** 的组装过程。

在 RLHF 的 PPO 阶段，模型生成的每个 Token 获得的奖励不仅仅是环境（Reward Model）给的打分，还需要减去一个 **KL 散度惩罚**，以防止模型生成的文本偏离由于微调（SFT）得到的参考模型（Reference Model）太远。

### 1. 核心数学公式

对于一个长度为 $T$ 的序列，第 $t$ 个 Token 的总奖励 $R_t$ 定义为：

$$ R_t = r_{\text{score}} \cdot \mathbb{I}(t=T) - \beta \cdot D_{KL}(\pi_\theta(\cdot|s_t) || \pi_{\text{ref}}(\cdot|s_t)) $$

其中：
*   **$r_{\text{score}}$**: 奖励模型（Reward Model）对整句生成的打分（这是一个标量，只在最后一个 Token 产生）。
*   **$\mathbb{I}(t=T)$**: 指示函数，表示只有在句子结束（EOS）位置才有 $r_{\text{score}}$，其他位置为 0。
*   **$\beta$** (`kl_coef`): KL 惩罚系数。
*   **$D_{KL}$**: 当前策略与参考策略的 KL 散度（近似为 $\log \pi_{\text{theta}} - \log \pi_{\text{ref}}$）。

---

### 2. 代码逐行解析

#### **Line 1: 函数定义**
```python
def compute_reward(r, kl):
```
*   `r`: 奖励模型给出的分数，通常是一个 Batch 的标量，形状 `[Batch_Size]`。
*   `kl`: 每个 Token 的 KL 散度值（或者 Log 概率差），形状 `[Batch_Size, Seq_Len]`。

#### **Line 2: 计算 KL 惩罚项**
```python
kl_reward = -kl_coef * kl
```
*   **对应公式**：$- \beta \cdot D_{KL}$
*   **含义**：这是**Dense Reward（稠密奖励）**。序列中的每一个 Token 都会收到这个“即时奖励”。因为是“惩罚”，所以前面加了**负号**。
    *   如果模型生成的 Token 概率远高于参考模型（偏离大，KL 大），`kl_reward` 就会是个很大的负数。

#### **Line 4: 寻找句子结束位置 (EOS Indices)**
```python
eos_indices = action_mask.size(1) - 1 - action_mask.long().fliplr().argmax(dim=1, keepdim=True)
```
这是一个非常经典的**PyTorch Trick**，用来在填充（Padding）后的 Tensor 中找到每句话的**最后一个有效 Token 的下标**。
*   假设 `action_mask` 为 `[1, 1, 1, 0, 0]` (长度 5，实际长度 3)。
*   `fliplr()` 翻转 $\rightarrow$ `[0, 0, 1, 1, 1]`。
*   `argmax(dim=1)` 找最大值（也就是 1）出现的第一个位置 $\rightarrow$ Index 为 `2`。
*   `size(1) - 1 - Index` $\rightarrow$ $5 - 1 - 2 = 2$。
*   原数组下标 `2` 确实是最后一个 `1` 的位置。

#### **Line 6: 构建稀疏奖励 (Sparse Reward)**
```python
last_reward = torch.zeros_like(kl).scatter_(dim=1, index=eos_indices, src=r.unsqueeze(1).to(kl.dtype))
```
*   **对应公式**：$r_{\text{score}} \cdot \mathbb{I}(t=T)$
*   **含义**：
    1.  `torch.zeros_like(kl)`: 创建一个全 0 的矩阵，形状与 `kl` 相同（`[Batch, Seq]`）。
    2.  `scatter_`: 将奖励模型的分数 `r` **“填空”** 到全 0 矩阵中。
    3.  `index=eos_indices`: 指定填空的位置（即每句话的结尾）。
    4.  **结果**：`last_reward` 变成了一个只有在 EOS 位置有值（例如 0.8），其他位置全为 0 的矩阵。

#### **Line 7: 奖励求和**
```python
reward = last_reward + kl_reward
```
*   **对应公式**：$R_{total} = R_{score} + R_{KL\_Penalty}$
*   **含义**：将稀疏的最终得分与每个步骤的 KL 惩罚相加，得到最终用于训练 PPO 的 Advantage 计算的 Reward 矩阵。
    *   **中间 Token** 的奖励 = $0 - \beta \cdot \text{KL}$
    *   **最后一个 Token** 的奖励 = $Score - \beta \cdot \text{KL}$

---

### 3. 关于 Line 8 注释的解释

```python
# 这里KL_reward计算是ref_logp - actor_logp，所以是+号...
```

这行注释是在解释**符号的正负问题**。

通常 KL 散度（近似）计算为：
$$ \text{kl} \approx \log \pi_{\text{actor}} - \log \pi_{\text{ref}} $$
这是一个**正数**。

1.  **代码逻辑 (Line 2)**:
    `kl_reward = -kl_coef * kl`。
    如果输入的 `kl` 是正数，乘以负系数后，`kl_reward` 变为**负数**（惩罚）。
    所以在 Line 7 使用 `+` 号连接：`Total = Score + (-Penalty)`。

2.  **注释的含义**:
    注释提到如果是 `ref_logp - actor_logp`。
    $$ \log \pi_{\text{ref}} - \log \pi_{\text{actor}} = - (\log \pi_{\text{actor}} - \log \pi_{\text{ref}}) \approx -\text{KL} $$
    这本身就是一个**负数**。
    如果输入的 `kl` 变量本身已经是这个负数值，那么 Line 2 的计算逻辑可能需要调整，或者注释只是在强调**这一项本身代表的是一种“负反馈”或“惩罚”**，因此在最终合成时，要确保数学上它是从总分里扣除的。

**总结**：无论注释怎么写，核心逻辑是不变的——**模型生成的越偏离参考模型，最终得到的 Total Reward 就越低。**
# DPO
$$
\mathcal{L}_{\text{DPO}}(\pi_{\theta}, \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_{\theta}(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right]
$$
```python
class DPO(nn.Module):
	"""
	DPO Loss
	"""
	def __init__(self, beta: float, label_smothing: float = 0.0, ipo: bool = False) -> None:
		super().__init__()
		self.beta = beta
		self.label_smoothing = label_smoothing
		self.ipo = ipo
		
	def forward(
		self, 
		policy_chosen_logps, 
		policy_rejected_logps,
		ref_chosen_logps,
		ref_rejected_logps,
	):
		pi_logratios = policy_chosen_logps - ref_chosen_logps
		ref_lograios = policy_rejected_logps - ref_rejected_logps
		logits = pi_logratios - ref_lograios
		losses = (
			-F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing)
			-F.logsigmoid(-self.beta * logits) * self.label_smoothing
		)
		loss = losses.mean()
		chosen_rewards = self.beta * pi_logratios.detach()
		rejected_rewards = self.beta * ref_lograios.detach()
		return loss, chosen_rewards, rejected_rewards
```

这是 **Direct Preference Optimization (DPO)** 的损失函数公式。DPO 是一种用于大语言模型（LLM）对齐的算法，它作为 RLHF（基于人类反馈的强化学习）的一种替代方案，因不需要单独训练奖励模型（Reward Model）且更稳定而广受关注。

### 1. 符号含义解释

*   **$\mathcal{L}_{\text{DPO}}$**: DPO 的损失值（Loss）。
*   **$\pi_{\theta}$**: 正在训练的模型（Policy Model）。
*   **$\pi_{\text{ref}}$**: 参考模型（Reference Model），通常是经过 SFT（监督微调）但未经过 RLHF 的原始模型。在训练过程中，它通常保持参数冻结。
*   **$\mathcal{D}$**: 偏好数据集（Dataset）。
*   **$(x, y_w, y_l)$**: 数据集中的样本三元组。
    *   $x$: 输入的提示词（Prompt）。
    *   $y_w$: 被人类或强模型标记为**获胜/更好**的回答（Winner/Preferred）。
    *   $y_l$: 被标记为**失败/较差**的回答（Loser/Rejected）。
*   **$\beta$**: 一个超参数（系数），用于控制模型偏离参考模型的程度（KL 散度约束的权重）。
*   **$\sigma$**: Sigmoid 函数，将数值映射到 $(0, 1)$ 区间。

### 3. 直观原理解释

这个公式的核心逻辑是**增大“好回答”与“坏回答”之间的评分差距**。

我们可以把 $\beta \log \frac{\pi_{\theta}(y|x)}{\pi_{\text{ref}}(y|x)}$ 理解为模型对于某个回答 $y$ 的**隐式奖励（Implicit Reward）**。

1.  **比率项 $\frac{\pi_{\theta}(y|x)}{\pi_{\text{ref}}(y|x)}$**:
    *   如果训练模型 $\pi_{\theta}$ 生成 $y$ 的概率比参考模型 $\pi_{\text{ref}}$ 高，这个比率就大于 1，log 值大于 0（奖励为正）。
    *   DPO 希望模型对于“好回答” $y_w$ 的生成概率相对于参考模型有所提升，而对“坏回答” $y_l$ 的生成概率有所下降。

2.  **差值项**:
    *   公式中间的减法计算的是：**（好回答的隐式奖励） - （坏回答的隐式奖励）**。
    *   我们希望这个差值越大越好。

3.  **Sigmoid ($\sigma$) 和 Log**:
    *   Sigmoid 将这个差值转换成一个概率值。
    *   最外层的 $-\log$ 意味着我们实际上是在做**最大似然估计**（Minimize Negative Log Likelihood）。
    *   **总结**：最小化这个 Loss，等同于最大化“好回答的奖励高于坏回答奖励”的概率。

**一句话总结：**
DPO 通过直接优化语言模型，使其在保持不偏离参考模型太远的前提下（由 $\pi_{\text{ref}}$ 和 $\beta$ 控制），尽可能地给“好回答”分配比“坏回答”更高的概率。它绕过了传统 PPO 算法中需要单独训练奖励模型的复杂步骤。

这段代码是 **DPO (Direct Preference Optimization)** 损失函数的 PyTorch 实现版本。它精确地对应了你之前提供的数学公式，并增加了一些工程上的优化（如 Label Smoothing）。

我将结合数学公式，逐行、逐块为你解释这段代码。

### 代码解释
#### 1. 初始化部分 (`__init__`)
**Line 1-9**
```python
class DPOLoss(nn.Module):
    def __init__(self, beta: float, label_smoothing: float = 0.0, ipo: bool = False) -> None:
        super().__init__()
        self.beta = beta
        self.label_smoothing = label_smoothing
        self.ipo = ipo
```
*   **`beta`**: 对应公式中的 $\beta$。这是一个超参数，控制对参考模型（Reference Model）的偏离程度。通常取值在 0.1 到 0.5 之间。
*   **`label_smoothing`**: **标签平滑**。公式里默认好回答是 1，坏回答是 0。平滑后，好回答可能是 0.9，坏回答是 0.1。这有助于防止模型过度自信，增加泛化能力。
*   **`ipo`**: 一个标志位。虽然代码里定义了它，但在下文的计算逻辑中（Line 20-23）使用的是 DPO 的 Sigmoid 形式。通常 IPO 算法会使用均方误差（MSE），这里暂未体现。

---

#### 2. 损失值计算 (Loss Calculation)
**Line 20-23**
```python
losses = (
    -F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing)
    - F.logsigmoid(-self.beta * logits) * self.label_smoothing
)
```
这里实现了 DPO 的核心 Loss 公式，并加入了 Label Smoothing。

*   **标准 DPO 部分** (当 `label_smoothing=0` 时)：
    *   只剩第一项：`-F.logsigmoid(self.beta * logits)`。
    *   这完全对应公式：$-\log \sigma (\dots)$。
    *   `logits` 就是上面算出来的差值，`self.beta` 就是 $\beta$。

*   **Label Smoothing 部分**：
    *   如果 `label_smoothing > 0`（比如 0.1），它不仅仅最小化“正向”的 Loss，还会加上一小部分“反向”的 Loss。
    *   `-F.logsigmoid(-self.beta * logits)`：这是在计算如果把“好回答”当成“坏回答”时的 Loss。
    *   目的：混合 90% 的正向目标和 10% 的反向目标，防止模型为了拉大分差而走向极端，导致数值不稳定。

**Line 24**
```python
loss = losses.mean()
```
对一个 Batch 内的所有样本求平均值，作为最终反向传播的 Loss。

---

#### 3. 奖励值追踪 (Metrics/Rewards)
**Line 25-26**
```python
chosen_rewards = self.beta * (policy_chosen_logps - reference_chosen_logps).detach()
rejected_rewards = self.beta * (policy_rejected_logps - reference_rejected_logps).detach()
```
*   这两行**不参与梯度下降**（注意 `.detach()`），它们纯粹是为了**监控训练过程**。
*   根据 DPO 论文推导，隐式奖励（Implicit Reward）定义为：$r(x,y) = \beta \log \frac{\pi_{\theta}(y|x)}{\pi_{\text{ref}}(y|x)}$。
*   代码正是计算了这个值。
*   **用途**：在训练日志（如 WandB）中，你会看到 `chosen_rewards` 逐渐上升，`rejected_rewards` 逐渐下降，这代表 DPO 训练起效了——模型更喜欢生成 $y_w$ 而非 $y_l$。

### 总结
这段代码通过以下步骤实现了 DPO：
1.  计算 Policy 模型和 Reference 模型在“好/坏”回答上的 Log 概率差。
2.  通过代数变换计算出 Advantage（即 `logits`）。
3.  应用 Sigmoid 和 Log 变换（`F.logsigmoid`）。
4.  结合 Label Smoothing 计算最终 Loss，同时记录隐式奖励用于监控。

# GRPO
GRPO (Group Relative Policy Optimization) 目标函数公式：

$$
\mathcal{J}_{GRPO}(\theta) = \mathbb{E}[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)] \\
\frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left\{ \min \left[ \frac{\pi_\theta(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q, o_{i,<t})} \hat{A}_{i,t}, \text{clip} \left( \frac{\pi_\theta(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\varepsilon, 1+\varepsilon \right) \hat{A}_{i,t} \right] - \beta \mathbb{D}_{KL}[\pi_\theta || \pi_{ref}] \right\}
$$
**公式中的关键部分解释：**
*   **$\mathcal{J}_{GRPO}(\theta)$**: GRPO 的目标函数。
*   **$\mathbb{E}[\dots]$**: 期望值，基于问题 $q$ 和旧策略生成的 $G$ 个输出 $o_i$。
*   **$\frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|}$**: 对每一个组（Group）内的每一个回答，以及回答中的每一个 Token 进行平均求和。
*   **$\min \left[ \dots \right]$**: 这是 PPO 算法核心的 Clipped Surrogate Objective，用于限制策略更新幅度。
*   **$\hat{A}_{i,t}$**: 优势函数（Advantage），这里采用了 Group 归一化的奖励估计。
*   **$- \beta \mathbb{D}_{KL}[\pi_\theta || \pi_{ref}]$**: KL 散度惩罚项，用于防止新策略 $\pi_\theta$ 偏离参考策略 $\pi_{ref}$ 太远。根据图片文字描述，这一项直接加在了策略优化项后面（即每一个 Token 的计算中），而不是作为奖励的一部分。
```python
import torch
import torch.nn as nn

class GRPOTrainer(nn.Module):
    def __init__(self, model, ref_model, beta=0.01, epsilon=0.2):
        super().__init__()
        self.model = model           # 当前策略网络 (Actor)
        self.ref_model = ref_model   # 参考网络 (Reference Model, 冻结参数)
        self.beta = beta             # KL 惩罚系数
        self.epsilon = epsilon       # PPO Clip 系数 (通常 0.2)

    def compute_loss(self, inputs):
        """
        inputs 包含:
            - input_ids: [B * G, Seq_Len] (Prompt + Completion)
            - attention_mask: [B * G, Seq_Len]
            - rewards: [B, G] (外部 Reward Model 计算得到的标量奖励)
            - old_logps: [B * G, Seq_Len] (采样时旧策略的 Log 概率，可选)
        """
        input_ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]
        rewards = inputs["rewards"] # 形状 [Batch_Size, Group_Size]
        
        # ---------------------------------------------------------------------
        # Step 1: 计算 Advantage (核心修改点 1)
        # ---------------------------------------------------------------------
        # 直接使用 Group 内的统计数据进行归一化，不需要 Value Model
        advantages = compute_group_advantages(rewards) 
        
        # 展平 advantages 以匹配 input_ids 的维度 [B * G]
        # 这里的 advantage 是句子级别的，同一个句子的每个 token 共享同一个 advantage
        advantages = advantages.view(-1) 

        # ---------------------------------------------------------------------
        # Step 2: 模型前向传播
        # ---------------------------------------------------------------------
        # 获取当前策略的 Log Probabilities
        outputs = self.model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        # 将 logits 转换为 log_softmax
        log_probs = F.log_softmax(logits, dim=-1)
        
        # 获取对应目标 token 的 log_prob
        # shift_logits用于预测下一个token，通常输入需要错位
        # 这里简化处理，假设 gather_log_probs 函数能正确提取 completion 部分的 logps
        per_token_logps = self.gather_log_probs(log_probs, input_ids, attention_mask)

        # 获取参考模型 (Ref Model) 的 Log Probabilities (用于计算 KL)
        with torch.no_grad():
            ref_outputs = self.ref_model(input_ids, attention_mask=attention_mask)
            ref_logits = ref_outputs.logits
            ref_log_probs = F.log_softmax(ref_logits, dim=-1)
            ref_per_token_logps = self.gather_log_probs(ref_log_probs, input_ids, attention_mask)

        # ---------------------------------------------------------------------
        # Step 3: 计算 KL 散度 (近似)
        # ---------------------------------------------------------------------
        # formula: exp(log_p - log_q) - (log_p - log_q) - 1  (Schulman et al. estimator)
        # 或者简单的: log_p - log_q (如果直接用 log ratio)
        # 这里使用 DeepSeek/TRL 常用的近似方式：
        kl_div = torch.exp(ref_per_token_logps - per_token_logps) - \
                 (ref_per_token_logps - per_token_logps) - 1

        # ---------------------------------------------------------------------
        # Step 4: PPO Clipping Loss 计算
        # ---------------------------------------------------------------------
        # 如果是第一次迭代，old_logps 等于当前 logps (ratio = 1)
        # 实际训练中通常会有多轮 epoch，这里假设 inputs 里有 old_logps
        old_per_token_logps = inputs.get("old_logps", per_token_logps.detach())

        # 计算概率比率 Ratio = pi_new / pi_old
        # log(a/b) = log(a) - log(b) => a/b = exp(log(a) - log(b))
        ratio = torch.exp(per_token_logps - old_per_token_logps)

        # 广播 advantages 到每个 token: [B*G] -> [B*G, Seq_Len]
        # 注意：mask 掉 prompt 部分，只计算 completion 部分的 loss
        completion_mask = inputs["completion_mask"] # 假设有这个mask
        
        # 扩展 advantage 维度以匹配 token 序列
        token_advantages = advantages.unsqueeze(1).expand_as(ratio)
        
        # 计算 clipped loss
        surr1 = ratio * token_advantages
        surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * token_advantages
        policy_loss = -torch.min(surr1, surr2) # 负号是因为要最小化 Loss

        # ---------------------------------------------------------------------
        # Step 5: 最终 Loss (核心修改点 2)
        # ---------------------------------------------------------------------
        # 根据 GRPO 公式，KL 散度是作为正则项加到 loss 里的
        # Loss = Policy_Gradient_Loss + beta * KL_Loss
        # 注意: 如果 advantages 是正数且很大，我们希望 ratio 变大。
        
        # 应用 Mask (只计算 Completion 部分的 loss)
        policy_loss = (policy_loss * completion_mask).sum() / completion_mask.sum()
        kl_loss = (kl_div * completion_mask).sum() / completion_mask.sum()

        total_loss = policy_loss + self.beta * kl_loss
        
        return total_loss, policy_loss, kl_loss

    def gather_log_probs(self, log_probs, input_ids, attention_mask):
        """
        辅助函数：提取对应 label 的 log probability
        通常相当于 torch.gather(log_probs, -1, input_ids.unsqueeze(-1))
        """
        # input_ids 作为 label (自回归)
        # 实际代码中通常需要对齐 input_ids[:, 1:] 和 logits[:, :-1]
        labels = input_ids[:, 1:].clone()
        logits = log_probs[:, :-1, :]
        
        per_token_logps = torch.gather(logits, -1, labels.unsqueeze(-1)).squeeze(-1)
        # 补齐长度以便后续计算
        return torch.cat([torch.zeros_like(per_token_logps[:, :1]), per_token_logps], dim=1)
	
```

## `compute_group_advantages` 方法实现

这个函数的核心思想是，对于同一个 Prompt 生成的多个回复（Group），我们希望通过**相对排序**来奖励那些比平均回复更好的回复，惩罚那些比平均回复差的回复。这种相对排序的优势函数，使得 GRPO 对奖励信号的尺度（Scale）和偏移（Shift）不敏感，因此不需要预先训练一个 Value Model 来估计状态值。

```python
import torch

def compute_group_advantages(rewards, epsilon=1e-4):
    """
    计算 GRPO 的优势函数 (Group Relative Advantages)
    
    Args:
        rewards: Tensor, shape [Batch_Size, Group_Size]
                 表示每个 prompt 生成的 G 个回答的原始奖励值。
                 Batch_Size 是原始 Batch 大小，Group_Size 是每个 prompt 采样的回复数量 (G)。
        epsilon: 一个小的正数，用于防止除以零（当所有奖励都相同时）。
        
    Returns:
        advantages: Tensor, shape [Batch_Size, Group_Size]
                    归一化后的优势值。
    """
    
    # 1. 计算每个 Batch 内所有回复的平均奖励
    # rewards.mean(dim=1, keepdim=True) 的结果形状是 [Batch_Size, 1]
    # keepdim=True 保证了输出的维度仍然是 2D，方便后续广播计算。
    # 这个均值作为 Baseline，代表了 Prompt 的平均生成质量。
    group_mean = rewards.mean(dim=1, keepdim=True)
    
    # 2. 计算每个 Batch 内所有回复奖励的标准差
    # rewards.std(dim=1, keepdim=True) 的结果形状也是 [Batch_Size, 1]
    # 标准差反映了回复奖励的波动程度。
    group_std = rewards.std(dim=1, keepdim=True)
    
    # 3. 计算归一化后的优势 (Standardization)
    # 优势 = (单个回复的奖励 - 该 Batch 的平均奖励) / (该 Batch 的标准差 + epsilon)
    # 这里的计算是：
    #   a) (rewards - group_mean): 得到每个回复相对于其均值的“偏差”。
    #   b) / (group_std + epsilon): 将偏差进行标准化，使其在相对意义上具有可比性。
    #       - 如果 group_std 很大（奖励波动大），则优势值会相对较小。
    #       - 如果 group_std 很小（奖励接近），则优势值会相对较大。
    #       - epsilon 用于防止 group_std 为 0 时出现除零错误。
    advantages = (rewards - group_mean) / (group_std + epsilon)
    
    return advantages

```

**工作流程演示：**

假设我们有一个 Batch，其中包含 2 个 Prompt ($Batch\_Size = 2$)。
每个 Prompt 我们生成了 4 个回复 ($Group\_Size = 4$)。

`rewards` 的形状是 `[2, 4]`：

```
# Example rewards tensor
# rewards = torch.tensor([
#     [1.0, 2.0, 0.5, 1.5],  # Prompt 1, Responses 1-4
#     [5.0, 6.0, 5.5, 7.0]   # Prompt 2, Responses 1-4
# ])
```

**计算过程：**

**对于 Prompt 1 (第一行):**
*   `rewards[0, :]` = `[1.0, 2.0, 0.5, 1.5]`
*   `group_mean[0]` = `(1.0 + 2.0 + 0.5 + 1.5) / 4` = `1.25`
*   `group_std[0]` = `std([1.0, 2.0, 0.5, 1.5])` ≈ `0.589`
*   `advantages[0, :]` = `([1.0, 2.0, 0.5, 1.5] - 1.25) / (0.589 + epsilon)`
    *   `advantages[0, 0]` = `(1.0 - 1.25) / 0.589` ≈ `-0.424` (比平均差)
    *   `advantages[0, 1]` = `(2.0 - 1.25) / 0.589` ≈ `1.273` (比平均好)
    *   `advantages[0, 2]` = `(0.5 - 1.25) / 0.589` ≈ `-1.273` (比平均差很多)
    *   `advantages[0, 3]` = `(1.5 - 1.25) / 0.589` ≈ `0.424` (比平均好)

**对于 Prompt 2 (第二行):**
*   `rewards[1, :]` = `[5.0, 6.0, 5.5, 7.0]`
*   `group_mean[1]` = `(5.0 + 6.0 + 5.5 + 7.0) / 4` = `5.875`
*   `group_std[1]` = `std([5.0, 6.0, 5.5, 7.0])` ≈ `0.744`
*   `advantages[1, :]` = `([5.0, 6.0, 5.5, 7.0] - 5.875) / (0.744 + epsilon)`
    *   `advantages[1, 0]` = `(5.0 - 5.875) / 0.744` ≈ `-1.176`
    *   `advantages[1, 1]` = `(6.0 - 5.875) / 0.744` ≈ `0.168`
    *   `advantages[1, 2]` = `(5.5 - 5.875) / 0.744` ≈ `-0.504`
    *   `advantages[1, 3]` = `(7.0 - 5.875) / 0.744` ≈ `1.512`

最终返回的 `advantages` Tensor 形状为 `[2, 4]`，包含了每个回复相对于其 Prompt 内其他回复的相对好坏程度。