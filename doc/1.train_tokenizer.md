## 1. train tokenizer

### 1.1 准备训练数据

首先要为模型训练一个tokenizer，训练数据作者已经准备好了，我们直接下载：

```bash
mkdir dataset
cd dataset
wget https://www.modelscope.cn/datasets/gongjy/minimind_dataset/resolve/master/pretrain_hq.jsonl
```

这个 jsonl 文件很大，在 linux 命令行查看前 2 条数据：

```bash
head -n 2 pretrain_hq.jsonl | jq .
```

命令行输出如下：

> 可以看到每一条数据都是由很多个 `<|im_start|>` 和 `<|im_end|>` 包裹的句子。

```bash
{
  "text": "<|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等。\n文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>"
}
{
  "text": "<|im_start|>根据输入的内容，编写一个类别标签。\n这是一篇介绍如何阅读心电图的文章类别标签: 医学/心电图阅读指南<|im_end|> <|im_start|>帮我搜索一下最近的天气情况。当然，我可以帮您搜索最新的天气情况。请问您需要查询哪个城市的天气情况呢？<|im_end|> <|im_start|>帮我讲一个令人开心的笑话。好的，我帮您讲一个关于细菌的笑话。为什么细菌不会上网？因为连接总是断开了！<|im_end|> <|im_start|>现在给我生成一首关于大海的五言诗。碧波万顷月满天，海天相接处天地间。波涛滚滚江山美，海鸟翱翔日月闲。<|im_end|> <|im_start|>谢谢你，这篇文章很有用。不客气，我很高兴能够为您提供帮助。如果您还有其他问题或需求，随时可以对我说。<|im_end|> <|im_start|>你好，我想下载一个视频编辑软件，你有什么推荐吗？您好！当然，有很多选择。您想要免费软件还是愿意付费？<|im_end|> <|im_start|>为什么我的程序不输出正确结果？可能是代码逻辑有误，或者输入数据有误，需要仔细调试代码逻辑和输入数据。<|im_end|> <|im_start|>谢谢你的回答。现在我想知道这场比赛的具体时间和地点。这场比赛的时间是北京时间10月4日，地点是上海。<|im_end|>"
}
```

### 1.2 实现 `train_tokenizer.py`

创建 scripts 文件夹，并在其中实现 `train_tokenizer.py` ，预期实现情况如下：

```python
def train_tokenizer():
    '''从 JSONL 数据集训练 BPE 分词器，定义特殊 Token，保存为标准格式。'''
    pass

def eval_tokenizer():
    pass

def main():
    train_tokenizer()
    eval_tokenizer()

if __name__ == "__main__":
    main()
```

> 这里补充解释一下 BPE：
>
> **BPE（Byte Pair Encoding，字节对编码）**
>
> 是一种子词分词算法，核心思想是：将文本拆分为 “最合理的子词单元”，而非固定的单词或单字，从而平衡词汇表大小、编码效率和未登录词（OOV）处理能力。它是目前大语言模型（如 GPT、BERT）最常用的分词方案之一。
>
> BPE 的本质是 “**从最小单元（字节 / 字符）开始，迭代合并最频繁的子串对**”，最终形成固定大小的词汇表。

详细实现见代码。

> 补充一下 `tokenizer_config.json` 中的  **`"chat_template"`**: 聊天格式的核心
>
> 这是一个非常强大且复杂的字段，它定义了如何将一个多轮对话的消息列表（`messages`）转换为模型能够理解的单一字符串输入。
>
> 它使用的是 **Jinja2 模板语言**。让我们来分解它的逻辑：
>
> **输入示例**:
>
> ```python
> messages = [
>  {"role": "system", "content": "你是一个 helpful 的助手。"},
>  {"role": "user", "content": "你好！"},
>  {"role": "assistant", "content": "你好！有什么我可以帮助你的吗？"}
> ]
> ```
>
> **模板逻辑**:
>
> 1. **工具调用前缀 (Tools Prefix)**:
>
>    ```jinja
>    {%- if tools %}
>        ...  # 如果提供了工具列表，则生成一段关于如何使用工具的系统提示
>    {%- else %}
>        ...  # 如果没有工具，则生成普通的系统提示
>    {%- endif %}
>    ```
>
>    - 这部分是为 **工具增强型对话（Tool-Augmented Generation）** 设计的。如果你的模型需要调用外部工具（如搜索引擎、计算器），就会在这里插入工具的描述和调用格式说明。
>    - 在你的例子中，`else` 分支会生效，它会检查 `messages` 列表的第一个消息是否是 `system` 角色。如果是，就使用其内容；如果不是，就提供一个默认的 `"You are a helpful assistant"` 提示。
>
> 2. **多轮工具调用处理 (Multi-step Tool Handling)**:
>
>    ```jinja
>    {%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}
>    {%- for message in messages[::-1] %}
>        ...  # 这段逻辑比较复杂，它尝试从后往前遍历消息，
>             # 找出用户的最后一个原始查询，忽略掉中间的工具调用和工具返回结果。
>             # 这在处理多轮工具调用对话时很有用。
>    {%- endfor %}
>    ```
>
>    - 这部分逻辑在简单的聊天中可能不会改变最终结果，但它为处理更复杂的、多轮工具调用的对话流程打下了基础。
>
> 3. **主消息循环 (Main Message Loop)**:
>
>    ```jinja
>    {%- for message in messages %}
>        {%- if message.role == "user" or (message.role == "system" and not loop.first) %}
>            {{- '<|im_start|>' + message.role + '\n' + content + '<|im_end|>\n' }}
>        {%- elif message.role == "assistant" %}
>            {{- '<|im_start|>' + message.role + '\n' + content }}
>            {%- if message.tool_calls %}
>                ...  # 如果助手消息包含工具调用，将其格式化为 <tool_call> 标签
>            {%- endif %}
>            {{- '<|im_end|>\n' }}
>        {%- elif message.role == "tool" %}
>            ...  # 将工具返回的结果格式化为 <tool_response> 标签，并包裹在 user 角色中
>        {%- endif %}
>    {%- endfor %}
>    ```
>
>    - 这是模板的主体，它遍历 `messages` 列表中的每一条消息，并根据其 `role`（`system`, `user`, `assistant`, `tool`）将其格式化为带有 `<|im_start|>` 和 `<|im_end|>` 标签的字符串块。
>    - 特别注意对 `assistant` 和 `tool` 角色的处理，它们会被嵌入特殊的 XML 标签（`<tool_call>`, `<tool_response>`），以便模型能识别出这是工具相关的内容。
>
> 4. **生成提示 (Generation Prompt)**:
>
>    ```jinja
>    {%- if add_generation_prompt %}
>        {{- '<|im_start|>assistant\n' }}
>        ...
>    {%- endif %}
>    ```
>
>    - 当 `apply_chat_template` 函数被调用时，如果 `add_generation_prompt=True`（默认通常为 `True`），它会在最后添加一个 `<|im_start|>assistant\n` 的前缀。
>    - 这个前缀是在告诉模型：“现在轮到你（assistant）来生成回答了，请从这里开始。”
>
> **应用到输入示例的输出结果**:
>
> ```
> <|im_start|>system
> 你是一个 helpful 的助手。<|im_end|>
> <|im_start|>user
> 你好！<|im_end|>
> <|im_start|>assistant
> 你好！有什么我可以帮助你的吗？<|im_end|>
> <|im_start|>assistant
> ```
>
> （最后一行是 `add_generation_prompt=True` 时添加的）
>
> ### 总结
>
> 这个 `config` 文件是你训练的分词器和 `transformers` 生态系统之间的桥梁。它不仅定义了分词器的基础行为和特殊 token，还通过 `chat_template` 这个字段，为你的模型提供了理解和生成结构化聊天对话的能力。
>
> 通过手动编写这个配置文件，你精确地控制了分词器的每一个细节，使其完美适配你的聊天模型的需求。这是一个非常专业和正确的做法。

### 1.3 训练 tokenizer

这部分主要是将所有的数据拆分成 byte，统计相邻两个 byte 出现的次数，选择出现次数最多的两个相邻 byte 进行合并，如此往复，直到词表大小剩余 6400。

使用下面的命令查看你的机器有多少个 cpu 核心：

```bash
 grep -c "processor" /proc/cpuinf
```

我的输出结果是 48 个核心，不过你也不用改什么，底层 rust 默认使用多线程处理。

运行训练脚本：

```bash
pixi run python scripts/train_tokenizer.py
```

此时我陷入漫长的等待…【其实我没等，直接开始写下一部分了( •̀ ω •́ )y】

```bash
[00:02:42] Pre-processing sequences       ████████████████████████████████████████████████████ 0        /        0
[00:02:27] Tokenize words                 ████████████████████████████████████████████████████ 28220806 / 28220806
[00:04:38] Count pairs                    ████████████████████████████████████████████████████ 28220806 / 28220806
[00:17:05] Compute merges                 ███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 395      /     6400

```

训练完测试tokenizer的输出如下：

```bash
<|im_start|>system
你是一个优秀的聊天机器人，总是给我正确的回应！<|im_end|>
<|im_start|>user
你来自哪里？<|im_end|>
<|im_start|>assistant
我来自地球<|im_end|>

tokenizer实际词表长度： 6400
encoder长度： 42
decoder和原始文本是否一致： True
```

至此，我们用预训练数据先训练了一个tokenizer【tokenizer.json + tokenizer_config.json】