## 11. GRPO

这段代码实现了一个基于 **GRPO (Group Relative Policy Optimization)** 算法的强化学习训练流程，特别针对具备“思维链（Chain of Thought, CoT）”能力的模型进行了优化。

GRPO 是一种去掉了 Critic（价值网络）的 PPO 变体，它通过对同一个 Prompt 生成的一组输出（Group）计算相对优势（Advantage），从而降低训练资源消耗。

下面分两部分详细解释代码逻辑及对应的数学公式。

---

### 第一部分：奖励计算 (`calculate_rewards`)

这个函数负责为生成的回答计算一个标量奖励值 $R$。奖励由两部分组成：**格式奖励（规则）** 和 **内容奖励（模型）**。

#### 1. 格式奖励 (Reasoning Format Reward)

针对推理模型，代码强制要求输出符合 XML 标签格式 `<think>...</think><answer>...</answer>`。

*   **严格格式匹配**：
    如果回答严格符合正则表达式（`<think>`...`</think>` 紧接 `<answer>`...`</answer>`），给予 **+0.5** 分。
*   **标签计数**：
    统计 `<think>`, `</think>`, `<answer>`, `</answer>` 这四个标签的出现次数。每个标签出现一次给予 **+0.25** 分。
    *   如果四个标签都齐全，这部分得 **+1.0** 分。

**格式总奖励公式**：

$$R_{format} = 0.5 \cdot \mathbb{I}_{pattern} + 0.25 \cdot \sum_{tag \in Tags} \mathbb{I}_{tag\_exists}$$

其中 $\mathbb{I}$ 是指示函数。

#### 2. 内容奖励 (Model-based Reward)

使用预训练的 `reward_model`（RM）来评估回答的质量。

*   **基础分数**：将 `(Prompt, Response)` 输入 RM 得到分数 $S_{full}$。
*   **推理优化（Reasoning Specific）**：
    如果开启了推理模式 (`args.reasoning == 1`)，代码会提取 `<answer>...</answer>` 中的内容，单独计算仅包含最终答案的奖励 $S_{answer}$。
    最终模型分数为加权和：
    $$R_{model} = 0.4 \cdot S_{full} + 0.6 \cdot S_{answer}$$
    *这表明算法更看重最终答案的质量，但也关注包含思维链的整体生成质量。*
    分数会被截断（Clip）在 $[-3.0, 3.0]$ 之间以防止极端值影响训练。

#### 3. 总奖励

$$R_{total} = R_{format} + R_{model}$$

---

### 第二部分：GRPO 训练循环 (`grpo_train_epoch`)

这是 GRPO 算法的核心实现。它摒弃了 PPO 中的 Value Network（Critic），而是利用同一组 Prompt 生成的多个样本的统计特性来估计基线（Baseline）。

#### 1. 采样 (Generation)

对于每个 Prompt $x$，模型 $\pi_\theta$ 生成 $G$ 个不同的回答 $\{y_1, y_2, ..., y_G\}$（代码中 $G$ 为 `args.num_generations`）。
$$\{y_1, ..., y_G\} \sim \pi_\theta(\cdot | x)$$

#### 2. 计算优势 (Advantage Computation)

这是 GRPO 的核心。不同于 PPO 使用 $A = R - V(s)$，GRPO 使用组内标准化。

代码逻辑：

1.  计算这 $G$ 个回答的奖励 $R = \{r_1, ..., r_G\}$。
2.  计算组内的均值 $\mu$ 和标准差 $\sigma$。
3.  计算优势 $A_i$：
    $$A_i = \frac{r_i - \text{mean}(\{r_1...r_G\})}{\text{std}(\{r_1...r_G\}) + \epsilon}$$
    *代码中还额外做了一次全局 Batch 级别的归一化，但这通常不是标准 GRPO 的必须步骤，可能是为了进一步稳定数值。*

#### 3. 损失函数 (Loss Function)

损失函数包含**策略梯度损失**和**KL 散度惩罚**。

**A. KL 散度 (KL Divergence):**
为了防止模型偏离参考模型（Ref Model）太远，计算当前策略 $\pi_\theta$ 和参考策略 $\pi_{ref}$ 之间的 KL 散度。
代码使用了一个近似公式（Schulman's estimator）：
$$D_{KL} \approx e^{(\log \pi_{ref} - \log \pi_\theta)} - (\log \pi_{ref} - \log \pi_\theta) - 1$$
令 $ratio_{ref} = \frac{\pi_{ref}}{\pi_\theta}$，则 $D_{KL} \approx ratio_{ref} - \log(ratio_{ref}) - 1$。
这是一个非负项，当且仅当 $\pi_\theta = \pi_{ref}$ 时为 0。

**B. 策略目标 (Policy Objective):**
代码中的 `per_token_loss` 计算如下：

$$\mathcal{L}_{token} = -\left( \frac{\pi_\theta(y|x)}{\pi_{\theta_{old}}(y|x)} \cdot A_i - \beta \cdot D_{KL} \right)$$

*注意：代码中使用了 `torch.exp(logp - logp.detach())`，这实际上就是 Importance Sampling Ratio，但在单次更新（不重复利用数据）的情况下，这个比率的值为 1，但保留了梯度传递。这简化为标准的策略梯度加上 KL 惩罚。*

**C. 最终损失:**
对生成的 Token 进行求和平均：
$$\mathcal{L} = \frac{1}{N_{tokens}} \sum_{t} \mathcal{L}_{token, t}$$

---

### 总结：GRPO 与 PPO 的区别

在这段代码中，体现了 GRPO 相比 PPO 的主要简化：

1.  **无 Critic 模型**：不需要训练一个 Value Function $V(s)$ 来估计 Baseline。
2.  **Group Baseline**：Baseline 直接来自于同一 Prompt 生成的 $G$ 个样本的平均奖励 $\frac{1}{G}\sum r_i$。
    *   如果一个回答 $r_i$ 比组内平均分高，那么 $A_i > 0$，模型会被鼓励生成该回答。
    *   如果比平均分低，模型会被抑制。

### 代码中的数学公式对应

1. **优势计算 (Line 107-109)**:

   ```python
   mean_r = grouped_rewards.mean(...)
   std_r = grouped_rewards.std(...)
   advantages = (rewards - mean_r) / (std_r + 1e-4)
   ```

   $$A_i = \frac{r_i - \mu}{\sigma}$$

2. **KL 惩罚 (Line 116-117)**:

   ```python
   kl_div = ref_per_token_logps - per_token_logps
   per_token_kl = torch.exp(kl_div) - kl_div - 1
   ```

   令 $D = \log \pi_{ref} - \log \pi_\theta$，则 $KL \approx e^D - D - 1$。

3. **最终 Loss (Line 118)**:

   ```python
   per_token_loss = -(torch.exp(...) * advantages - args.beta * per_token_kl)
   ```

   $$\text{Maximize} \quad \mathbb{E} \left[ \text{ratio} \cdot A - \beta \cdot D_{KL} \right]$$
   (代码中取负号是因为 PyTorch 默认做最小化)

### 实验结果


![image-20251215132927232](assets/11.GRPO/image-20251215132927232.png)
