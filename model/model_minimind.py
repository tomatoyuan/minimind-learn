# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MiniMind Config
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

from transformers import PretrainedConfig


class MiniMindConfig(PretrainedConfig):
    """
    MiniMind æ¨¡å‹çš„é…ç½®ç±»ã€‚
    ç»§æ‰¿è‡ª Hugging Face çš„ PretrainedConfigï¼Œæ”¯æŒæ¨¡å‹çš„é…ç½®ä¿å­˜ã€åŠ è½½ä»¥åŠä¸ AutoConfig å…¼å®¹ã€‚
    è¯¥é…ç½®æ¶µç›–äº†æ ‡å‡† Transformer å‚æ•°ã€RoPE (æ—‹è½¬ä½ç½®ç¼–ç ) æ‰©å±•è®¾ç½®ä»¥åŠ MoE (æ··åˆä¸“å®¶) ä¸“ç”¨è®¾ç½®ã€‚
    """
    model_type = "minimind" # æ¨¡å‹ç±»å‹æ ‡è¯†ç¬¦ï¼Œç”¨äº AutoModel è‡ªåŠ¨è¯†åˆ«

    def __init__(
            self,
            # === åŸºç¡€ Transformer å‚æ•° ===
            dropout: float = 0.0,            # Dropout æ¦‚ç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ (é€šå¸¸é¢„è®­ç»ƒä¸º0ï¼Œå¾®è°ƒå¯å¼€å¯)
            bos_token_id: int = 1,           # åºåˆ—å¼€å§‹ Token çš„ ID (Begin of Sentence)
            eos_token_id: int = 2,           # åºåˆ—ç»“æŸ Token çš„ ID (End of Sentence)
            hidden_act: str = 'silu',        # éšè—å±‚æ¿€æ´»å‡½æ•°ï¼Œè¿™é‡Œä½¿ç”¨ SiLU (Swish)ï¼Œæ˜¯ LLaMA ç­‰ç°ä»£ LLM çš„æ ‡é…
            hidden_size: int = 512,          # æ¨¡å‹åµŒå…¥ç»´åº¦ (d_model)ï¼Œå†³å®šäº†æ¨¡å‹çš„å®½åº¦
            intermediate_size: int = None,   # FFN (å‰é¦ˆç¥ç»ç½‘ç»œ) çš„ä¸­é—´å±‚ç»´åº¦ï¼Œé€šå¸¸æ˜¯ hidden_size çš„ 2-4 å€(å¸¸ç”¨ 8/3 å€)
            max_position_embeddings: int = 32768, # æ¨¡å‹èƒ½å¤„ç†çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ (Context Window)
            num_attention_heads: int = 8,    # Query (æŸ¥è¯¢) çš„æ³¨æ„åŠ›å¤´æ•°
            num_hidden_layers: int = 8,      # Transformer Block çš„å±‚æ•° (æ·±åº¦)
            
            # === GQA (Grouped Query Attention) å‚æ•° ===
            num_key_value_heads: int = 2,    # Key å’Œ Value çš„å¤´æ•°ã€‚
                                             # è‹¥æ­¤å€¼ < num_attention_headsï¼Œåˆ™è¡¨ç¤ºå¼€å¯ GQAã€‚
                                             # ä½œç”¨ï¼šå¤§å¹…é™ä½æ¨ç†æ˜¾å­˜å ç”¨å’Œ KV Cache å¤§å°ï¼Œæå‡æ¨ç†é€Ÿåº¦ã€‚
            
            vocab_size: int = 6400,          # è¯è¡¨å¤§å°ï¼Œå†³å®š Embedding å±‚çš„è¡Œæ•°
            rms_norm_eps: float = 1e-05,     # RMSNorm çš„åˆ†æ¯ä¿®æ­£é¡¹ (epsilon)ï¼Œé˜²æ­¢é™¤é›¶é”™è¯¯
            
            # === RoPE (æ—‹è½¬ä½ç½®ç¼–ç ) ç›¸å…³å‚æ•° ===
            rope_theta: int = 1000000.0,     # RoPE çš„åŸºé¢‘å‚æ•°ã€‚æ•°å€¼è¶Šå¤§ï¼Œæ¨¡å‹å¯¹é•¿è·ç¦»ä¾èµ–çš„è¡°å‡è¶Šæ…¢ï¼Œè¶Šé€‚åˆé•¿æ–‡æœ¬ã€‚
            inference_rope_scaling: bool = False, # å¼€å…³ï¼šæ˜¯å¦åœ¨æ¨ç†é˜¶æ®µå¼€å¯ RoPE ç¼©æ”¾ (ç”¨äºå¤–æ¨é•¿åº¦)
            
            # === ç¡¬ä»¶åŠ é€Ÿ ===
            flash_attn: bool = True,         # å¼€å…³ï¼šæ˜¯å¦ä½¿ç”¨ Flash Attention 2 åŠ é€Ÿè®¡ç®— (éœ€ç¡¬ä»¶æ”¯æŒ)

            ####################################################
            # MOE (æ··åˆä¸“å®¶æ¨¡å‹) ä¸“å±é…ç½®åŒºåŸŸ
            # åªæœ‰å½“ use_moe ä¸º True æ—¶ï¼Œä»¥ä¸‹å‚æ•°æ‰ä¼šåœ¨æ¨¡å‹é€»è¾‘ä¸­ç”Ÿæ•ˆ
            ####################################################
            use_moe: bool = False,           # ä¸»å¼€å…³ï¼šæ˜¯å¦å¯ç”¨ MoE æ¶æ„
            num_experts_per_tok: int = 2,    # Top-Kï¼šæ¯ä¸ª Token åœ¨æ¨ç†æ—¶æ¿€æ´»çš„ä¸“å®¶æ•°é‡ (é€šå¸¸è¿œå°äºæ€»ä¸“å®¶æ•°)
            n_routed_experts: int = 4,       # è·¯ç”±ä¸“å®¶ (Routed Experts) çš„æ€»æ•°ï¼šå¾…é€‰ä¸“å®¶çš„æ€»æ± å­å¤§å°
            n_shared_experts: int = 1,       # å…±äº«ä¸“å®¶ (Shared Experts) çš„æ•°é‡ï¼š
                                             # å€Ÿé‰´ DeepSeek-V2/V3 è®¾è®¡ï¼Œè¿™äº›ä¸“å®¶æ€»æ˜¯è¢«æ¿€æ´»ï¼Œä¸å‚ä¸è·¯ç”±ç«äº‰ï¼Œç”¨äºæ•è·é€šç”¨çŸ¥è¯†ã€‚
            scoring_func: str = 'softmax',   # é—¨æ§ç½‘ç»œ (Gate) è®¡ç®—è·¯ç”±æƒé‡çš„å‡½æ•°ï¼Œé€šå¸¸ä¸º softmax
            aux_loss_alpha: float = 0.1,     # è¾…åŠ©æŸå¤± (Auxiliary Loss) ç³»æ•°ï¼š
                                             # ç”¨äºè®­ç»ƒæ—¶å¹³è¡¡å„ä¸ªä¸“å®¶çš„è´Ÿè½½ï¼Œé˜²æ­¢ "ä¸“å®¶åå¡Œ" (Expert Collapse)ã€‚
            seq_aux: bool = True,            # è¾…åŠ©æŸå¤±è®¡ç®—æ–¹å¼ï¼šTrue è¡¨ç¤ºåœ¨åºåˆ—çº§åˆ«ç»Ÿè®¡è´Ÿè½½ï¼ŒFalse è¡¨ç¤ºåœ¨ Batch çº§åˆ«ç»Ÿè®¡ã€‚
            norm_topk_prob: bool = True,     # æ˜¯å¦å¯¹é€‰å‡ºçš„ Top-K ä¸“å®¶çš„æƒé‡è¿›è¡Œå½’ä¸€åŒ– (ä½¿å…¶å’Œä¸º1)ã€‚
            **kwargs                         # æ¥æ”¶å…¶ä»–æœªæ˜¾å¼å®šä¹‰çš„å‚æ•°å¹¶ä¼ ç»™çˆ¶ç±»
    ):
        super().__init__(**kwargs)
        
        # å°†ä¼ å…¥çš„å‚æ•°ç»‘å®šåˆ°å®ä¾‹å±æ€§ä¸Š
        self.dropout = dropout
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id
        self.hidden_act = hidden_act
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.num_key_value_heads = num_key_value_heads
        self.vocab_size = vocab_size
        self.rms_norm_eps = rms_norm_eps
        self.rope_theta = rope_theta
        self.inference_rope_scaling = inference_rope_scaling

        # === YaRN (Yet another RoPE extensioN) é…ç½®é€»è¾‘ ===
        # è¿™æ˜¯ä¸€ä¸ªç”¨äºé•¿åº¦å¤–æ¨ (Extrapolation) çš„æŠ€æœ¯ã€‚
        # å…è®¸æ¨¡å‹åœ¨æ¨ç†æ—¶å¤„ç†æ¯”è®­ç»ƒé•¿åº¦ (original_max_position_embeddings) æ›´é•¿çš„æ–‡æœ¬ã€‚
        # è¿™é‡Œè®¾ç½®ä¸ºå¦‚æœå¼€å¯ inference_rope_scalingï¼Œåˆ™åº”ç”¨ YaRN é…ç½®ï¼Œæ‰©å±•å€æ•°ä¸º 4 å€ã€‚
        self.rope_scaling = {
            "beta_fast": 4,      # YaRN é«˜é¢‘è¡°å‡å‚æ•°
            "beta_slow": 1,      # YaRN ä½é¢‘è¡°å‡å‚æ•°
            "factor": 4,         # çº¿æ€§æ‰©å±•å› å­ (Context window * 4)
            "original_max_position_embeddings": 2048, # æ¨¡å‹åŸå§‹é¢„è®­ç»ƒæ—¶çš„æœ€å¤§é•¿åº¦
            "type": "yarn"       # æŒ‡å®šç¼©æ”¾ç±»å‹ä¸º yarn
        } if self.inference_rope_scaling else None
        
        self.flash_attn = flash_attn

        ####################################################
        # MoE å‚æ•°ç»‘å®š
        ####################################################
        self.use_moe = use_moe
        self.num_experts_per_tok = num_experts_per_tok  # Top-K
        self.n_routed_experts = n_routed_experts        # ä¸“å®¶æ± å¤§å°
        self.n_shared_experts = n_shared_experts        # å…±äº«ä¸“å®¶æ•°
        self.scoring_func = scoring_func
        self.aux_loss_alpha = aux_loss_alpha            # è´Ÿè½½å‡è¡¡ Loss æƒé‡
        self.seq_aux = seq_aux
        self.norm_topk_prob = norm_topk_prob



# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MiniMind Model
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, List, Union
from transformers.activations import ACT2FN
from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig
from transformers.modeling_outputs import CausalLMOutputWithPast

class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        # å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•° gamma (weight)
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        # rsqrt æ˜¯ 1/sqrt(x)
        # pow(2).mean(-1) è®¡ç®— x^2 çš„å‡å€¼
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        # å¼ºåˆ¶è½¬ä¸º float32 è®¡ç®— norm ä»¥ä¿è¯æ•°å€¼ç²¾åº¦ï¼Œæœ€åå†è½¬å› x çš„ç±»å‹ (å¦‚ float16)
        return self.weight * self._norm(x.float()).type_as(x)

def precompute_freqs_cis(dim: int, end: int = int(32 * 1024), rope_base: float = 1e6,
                         rope_scaling: Optional[dict] = None):
    # 1. åŸºç¡€é¢‘ç‡è®¡ç®—ï¼ˆÎ¸ï¼‰
    freqs = 1.0 / (rope_base ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))

    # 2. YaRN æ‰©å±•ç®—æ³•ï¼ˆé•¿æ–‡æœ¬å¤–æ¨é€»è¾‘ï¼‰
    if rope_scaling is not None:
        # è·å–é…ç½®å‚æ•°
        original_max_position_embeddings = rope_scaling.get("original_max_position_embeddings", 2048) # 
        factor = rope_scaling.get("factor", 4) # æ‰©å±•å€æ•°
        beta_fast = rope_scaling.get("beta_fast", 4.0)
        beta_slow = rope_scaling.get("beta_slow", 1.0)

        # ä»…å½“æ¨ç†é•¿åº¦ end è¶…è¿‡è®­ç»ƒé•¿åº¦ orig_max æ—¶è§¦å‘
        if end / original_max_position_embeddings > 1:
            # å¯»æ‰¾é«˜é¢‘å’Œä½é¢‘çš„åˆ†ç•Œç‚¹ corr_dim
            # åç»­ä¼šæˆªå– freqs[0:corr_dim] å¯¹åº”çš„é¢‘ç‡åˆ†é‡ï¼ˆå³æœ‰æ•ˆä½é¢‘ç‡éƒ¨åˆ†ï¼‰ï¼Œè¿‡æ»¤æ‰é«˜é¢‘å™ªå£°æˆ–å†—ä½™çš„é«˜é¢‘åˆ†é‡
            corr_dim = next((i for i in range(dim // 2) if 2 * math.pi / freqs[i] > original_max_position_embeddings), dim // 2)

            # è®¡ç®—æ’å€¼æ–œå¡ (Ramp function)
            power = torch.arange(0, dim // 2, device=freqs.device).float() / max(dim // 2 - 1, 1)
            beta = beta_slow + (beta_fast - beta_slow) * power
            
            # YaRN ç¼©æ”¾å…¬å¼
            # å¯¹é«˜é¢‘åˆ†é‡ï¼ˆdim è¾ƒå°éƒ¨åˆ†ï¼‰ä¸æ€ä¹ˆç¼©æ”¾ï¼Œå¯¹ä½é¢‘åˆ†é‡ï¼ˆdim è¾ƒå¤§éƒ¨åˆ†ï¼‰è¿›è¡Œå¼ºç¼©æ”¾
            scale = torch.where(torch.arange(dim // 2, device=freqs.device) < corr_dim, 
                                (beta * factor - beta + 1) / (beta * factor), 
                                1.0 / factor)
            freqs = freqs * scale # ä¿®æ­£é¢‘ç‡

    # 3. ç”Ÿæˆä½ç½®ç¼–ç 
    t = torch.arange(end, device=freqs.device)
    # å¤–ç§¯ï¼šç”Ÿæˆ [seq_len, dim//2] çš„è§’åº¦çŸ©é˜µ theta * position
    freqs = torch.outer(t, freqs).float()
    
    # 4. æ‹¼æ¥ Cos å’Œ Sin
    # æ³¨æ„ï¼šè¿™é‡Œæ‹¼æ¥äº†ä¸¤æ¬¡ï¼Œæ˜¯ä¸ºäº†é€‚é…ä¸‹é¢çš„ rotate_half å®ç°
    # å½¢çŠ¶å˜ä¸º [seq_len, dim]
    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1)
    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1)
    return freqs_cos, freqs_sin

def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    '''
        !!!!!!! æ­¤å¤„å‘é‡çš„ç»´åº¦æš‚ä¸ç¡®å®šï¼Œç­‰åç»­debugæ—¶å†ç¡®è®¤!!!!!!

        åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰åˆ°æŸ¥è¯¢å‘é‡ q å’Œé”®å‘é‡ kã€‚
        
        å‚æ•°:
        - q: æŸ¥è¯¢å‘é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, seq_len, num_heads, head_dim]
        - k: é”®å‘é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, seq_len, num_heads, head_dim]
        - cos: é¢„è®¡ç®—çš„ä½™å¼¦é¢‘ç‡ï¼Œå½¢çŠ¶ä¸º [seq_len, head_dim]
        - sin: é¢„è®¡ç®—çš„æ­£å¼¦é¢‘ç‡ï¼Œå½¢çŠ¶ä¸º [seq_len, head_dim]
        - position_ids: ä½ç½®ç´¢å¼•ï¼Œå½¢çŠ¶ä¸º [batch_size, seq_len]
        
        è¿”å›:
        - q_embed: ç¼–ç åçš„æŸ¥è¯¢å‘é‡
        - k_embed: ç¼–ç åçš„é”®å‘é‡
    '''
    def rotate_half(x):
        '''è¾…åŠ©å‡½æ•°ï¼šå°†å‘é‡åˆ‡åˆ†ä¸ºä¸¤åŠï¼Œå¹¶äº¤æ¢é¡ºåºã€å–è´Ÿ
           [x1, x2] -> [-x2, x1]
        '''
        return torch.cat((-x[..., x.shape[-1] // 2:], x[..., :x.shape[-1] // 2]), dim=-1)

    # åº”ç”¨æ¬§æ‹‰å…¬å¼çš„å®æ•°å½¢å¼
    # q_embed = q * cos + rotate_half(q) * sin
    # å¼ é‡å½¢çŠ¶å˜åŒ–ï¼š
    # q: [batch_size, seq_len, num_heads, head_dim]
    # k: [batch_size, seq_len, num_heads, head_dim]
    # cos/sin: [seq_len, head_dim]
    # unsqueeze_dim: 1 è¡¨ç¤ºåœ¨ seq_len ç»´åº¦ä¸Šæ‰©å±•ï¼Œå°† cos/sin æ‰©å±•ä¸º [seq_len, 1, head_dim]
    # å¹¿æ’­æœºåˆ¶ï¼šè®¡ç®—æ—¶è‡ªåŠ¨å°†æœ€å·¦ä¾§ç»´åº¦å¯¹é½ï¼Œå°† cos/sin æ‰©å±•ä¸º [batch_size, seq_len, 1, head_dim]
    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(q) * sin.unsqueeze(unsqueeze_dim))
    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(k) * sin.unsqueeze(unsqueeze_dim))
    return q_embed, k_embed

def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    é‡å¤é”®å€¼å¼ é‡çš„æœ€åä¸€ä¸ªç»´åº¦ n_rep æ¬¡ã€‚

    å‚æ•°:
    - x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, seq_len, num_heads, head_dim]
    - n_rep: é‡å¤æ¬¡æ•°

    è¿”å›:
    - é‡å¤åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, seq_len, num_heads, head_dim * n_rep]
    """
    batch_size, seq_len, num_heads, head_dim = x.shape
    if n_rep == 1:
        return x
    # Noneï¼šç­‰ä»·äºnp.newaxisï¼Œä½œç”¨æ˜¯åœ¨æŒ‡å®šä½ç½®æ’å…¥ä¸€ä¸ªé•¿åº¦ä¸º 1 çš„æ–°ç»´åº¦ï¼ˆè¿™é‡Œæ’å…¥åœ¨ç¬¬ 3 ç»´ï¼‰ï¼Œä¸‹é¢ä¸¾ä¾‹ç¤ºæ„ç»´åº¦å˜åŒ–
    # (2,10,8,64) â†’ (2,10,8,1,64) â†’                 (2,10,8,2,64)                     â†’ (2,10,8,128)
    return x[:, :, :, None, :].expand(batch_size, seq_len, num_heads, n_rep, head_dim).reshape(batch_size, seq_len, num_heads, head_dim * n_rep)

class Attention(nn.Module):
    def __init__(self, args: MiniMindConfig):
        super().__init__()
        self.num_key_value_heads = args.num_attention_heads if args.num_key_value_heads is None else args.num_key_value_heads
        assert args.num_attention_heads % self.num_key_value_heads == 0, "num_attention_heads must be divisible by num_key_value_heads"
        self.n_local_heads = args.num_key_value_heads
        self.n_local_kv_heads = args.num_key_value_heads
        self.n_rep = self.n_local_heads // self.n_local_kv_heads
        self.head_dim = args.hidden_size // args.num_attention_heads
        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)
        self.attn_dropout = nn.Dropout(args.dropout)
        self.resid_dropout = nn.Dropout(args.dropout)
        self.dropout = args.dropout
        # hasattr(object, name) â€”â€” Python å†…ç½®å‡½æ•°: åˆ¤æ–­ä¸€ä¸ªå¯¹è±¡ï¼ˆobjectï¼‰æ˜¯å¦å…·æœ‰æŒ‡å®šåç§°ï¼ˆnameï¼‰çš„å±æ€§æˆ–æ–¹æ³•ï¼Œè¿”å›å¸ƒå°”å€¼
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn
    
    def forward(self,
                x: torch.Tensor,
                position_embeddings: Tuple[torch.Tensor, torch.Tensor], # ä¿®æ”¹ä¸ºæ¥æ”¶ cos å’Œ sin
                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                use_cache=False,
                attention_mask: Optional[torch.Tensor] = None
                ):
        bsz, seq_len, _ = x.shape
        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)
        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)
        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)

        cos, sin = position_embeddings
        xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])

        # kv_cache å®ç°
        if past_key_value is not None:
            # åœ¨ seq_len ç»´åº¦ä¸Šæ‹¼æ¥ç¼“å­˜çš„é”®å€¼å¯¹
            xk = torch.cat([past_key_value[0], xk], dim=1)
            xv = torch.cat([past_key_value[1], xv], dim=1)
        past_kv = (xk, xv) if use_cache else None

        xq, xk, xv = (
            xq.transpose(1, 2),
            repeat_kv(xk, self.n_rep).transpose(1, 2),
            repeat_kv(xv, self.n_rep).transpose(1, 2)
        )

        if self.flash and seq_len > 1:
            # åªæœ‰ â€œå¯ç”¨äº† Flash Attentionâ€ ä¸” â€œåºåˆ—é•¿åº¦æœ‰æ•ˆï¼ˆ>1ï¼‰â€ï¼Œæ‰è¿›å…¥åç»­æ©ç å¤„ç†é€»è¾‘ï¼›å¦åˆ™è·³è¿‡ï¼ˆä½¿ç”¨æ™®é€šæ³¨æ„åŠ›çš„æ©ç é€»è¾‘ï¼‰ã€‚
            if attention_mask is None or torch.all(attention_mask == 1):
                # ç”¨æˆ·æœªä¼ å…¥ä»»ä½•æ³¨æ„åŠ›æ©ç ï¼ˆé»˜è®¤æ— æ©ç ï¼‰æˆ–è€…ä¼ å…¥çš„æ©ç æ˜¯ â€œå…¨ 1 æ©ç â€ï¼ˆæ‰€æœ‰ä½ç½®éƒ½å…è®¸æ³¨æ„åŠ›åŠ æƒï¼Œç­‰ä»·äºæ— æ©ç ï¼‰ï¼›
                # åˆ™å°† attn_mask è®¾ç½®ä¸º Noneï¼ˆè¡¨ç¤ºæ— æ©ç ï¼‰ï¼Œå¹¶å°† is_causal è®¾ç½®ä¸º Trueï¼ˆè¡¨ç¤ºä½¿ç”¨å› æœæ©ç ï¼‰ã€‚
                attn_mask, is_causal = None, True
            else:
                # ç”¨æˆ·ä¼ å…¥äº†éå…¨ 1 çš„attention_maskï¼ˆæ¯”å¦‚æŸäº›ä½ç½®æ˜¯ 0ï¼Œéœ€è¦å±è”½è¿™äº›ä½ç½®çš„æ³¨æ„åŠ›ï¼‰ã€‚
                # torch.triuä¼šå°†éä¸Šä¸‰è§’éƒ¨åˆ†è®¾ä¸º 0ï¼Œè€Œä¸Šä¸‰è§’éƒ¨åˆ†ä¿ç•™åŸ-infï¼Œæœ€ç»ˆå®ç° â€œæœªæ¥ä½ç½®æ©ç ä¸º-infï¼Œå½“å‰å’Œè¿‡å»ä½ç½®ä¸º 0â€ã€‚
                causal_mask = torch.triu(torch.full((seq_len, seq_len), float('-inf'), device=xq.device), diagonal=1)
                # å½“å¯ç”¨ Flash Attention ä¸”æœ‰ç”¨æˆ·æŒ‡å®šçš„éå…¨ 1 æ©ç æ—¶ï¼ŒæŠŠ â€œå±è”½æœªæ¥ä½ç½®â€ çš„å› æœçº¦æŸå’Œ â€œå±è”½ç”¨æˆ·æŒ‡å®šä½ç½®â€ çš„æ™®é€šçº¦æŸï¼Œåˆå¹¶æˆä¸€ä¸ª Flash Attention èƒ½è¯†åˆ«çš„é«˜ç»´æ©ç ï¼ŒåŒæ—¶ç¦ç”¨å†…ç½®å› æœæ©ç é¿å…é‡å¤å¤„ç†ã€‚
                extended_mask = (1.0 - attention_mask.unsqueeze(1).unsqueeze(2)) * float('-inf')
                attn_mask, is_causal = causal_mask.unsqueeze(0) + extended_mask, False

            dropout_p = self.dropout if self.training else 0.0
            output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=attn_mask, dropout_p=dropout_p, is_causal=is_causal)
        else:
            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
            # å› æœæ©ç 
            scores = scores + torch.triu(
                torch.full((seq_len, seq_len), float("-inf"), device=scores.device),
                diagonal=1
            ).unsqueeze(0).unsqueeze(0)  # scores+mask

            # ç”¨æˆ·æŒ‡å®šçš„å±è”½ä½ç½®ï¼Œæ¯”å¦‚ padding
            if attention_mask is not None:
                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9
                scores = scores + extended_attention_mask

            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            scores = self.attn_dropout(scores)
            output = scores @ xv

        output = output.transpose(1, 2).contiguous().view(bsz, seq_len, -1)
        output = self.o_proj(output)
        output = self.resid_dropout(output)
        return output, past_kv
    
class FeedForward(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        if config.intermediate_size is None:
            intermediate_size = int(config.hidden_size * 8 / 3)
            config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)
        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)
        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        self.dropout = nn.Dropout(config.dropout)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        return self.dropout(self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x)))
    
class MoEFeedForward(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        pass

class MiniMindBlock(nn.Module):
    def __init__(self, layer_id: int, config: MiniMindConfig):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.hidden_size = config.hidden_size
        self.head_dim = config.hidden_size // config.num_attention_heads
        self.self_attn = Attention(config)

        self.layer_id = layer_id
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.mlp = FeedForward(config) if not config.use_moe else MoEFeedForward(config)
    
    def forward(self, hidden_states, position_embeddings, past_key_value=None, use_cache=False, attention_mask=None):
        # self attention
        residual = hidden_states
        hidden_states, present_key_value = self.self_attn(
            self.input_layernorm(hidden_states),
            position_embeddings,
            past_key_value=past_key_value,
            use_cache=use_cache,
            attention_mask=attention_mask
        )
        hidden_states = residual + hidden_states
        # FFN
        hidden_states = hidden_states + self.mlp(self.post_attention_layernorm(hidden_states))
        return hidden_states, present_key_value
    

class MiniMindModel(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config
        self.vocab_size = config.vocab_size
        self.num_hidden_layers = config.num_hidden_layers
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.dropout = nn.Dropout(config.dropout)
        self.layers = nn.ModuleList([MiniMindBlock(i, config) for i in range(config.num_hidden_layers)])
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        freqs_cos, freqs_sin = precompute_freqs_cis(dim=config.hidden_size // config.num_attention_heads,
                                                    end=config.max_position_embeddings,
                                                    rope_base=config.rope_theta,
                                                    rope_scaling=config.rope_scaling)
        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: Optional[bool] = False,
                **kwargs
                ):
        batch_size, seq_len = input_ids.shape
        # å…ˆæ¸…ç©ºä¸€ä¸‹ past_key_values é˜²æ­¢å‡ºé”™
        if hasattr(past_key_values, 'layers'): past_key_values = None
        past_key_values = past_key_values or [None] * self.num_hidden_layers
        # past_key_values: List of Tuples, æ¯ä¸ª Tuple æ˜¯æ¯ä¸€ä¸ªblockå±‚çš„ (key, value)
        # shape: (batch_size, past_seq_len, num_kv_heads, head_dim)
        # start_pos å¯¹åº”ä¸‹ä¸€ä¸ªè¾“å…¥ token çš„ä½ç½®ç´¢å¼•ï¼Œä¹‹å‰å¯¹è¯çš„å†…å®¹
        start_pos = past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0

        hidden_states = self.embed_tokens(input_ids)  # [bsz, seq_len, hidden_size]
        hidden_states = self.dropout(hidden_states)

        position_embeddings = (
            # seq_len å¯¹åº”å½“å‰è¾“å…¥çš„é•¿åº¦
            self.freqs_cos[start_pos: start_pos + seq_len],
            self.freqs_sin[start_pos: start_pos + seq_len]
        )

        presents = [] # å½“å‰çš„ KV
        for layer_idx, (layer, past_key_value) in enumerate(zip(self.layers, past_key_values)):
            # é€ä¸ª block è¿›è¡Œè¿ç®—
            hidden_states, present = layer(
                hidden_states,
                position_embeddings,
                past_key_value=past_key_value,
                use_cache=use_cache,
                attention_mask=attention_mask
            )
            presents.append(present)
        
        hidden_states = self.norm(hidden_states)

        # è®¡ç®— MoE è¾…åŠ©æŸå¤±
        aux_loss = sum(
            layer.mlp.aux_loss for layer in self.layers if hasattr(layer.mlp, MoEFeedForward)
        )

        return hidden_states, presents, aux_loss


class MiniMindForCausalLM(PreTrainedModel, GenerationMixin):
    # ç»§æ‰¿ PreTrainedModel åï¼Œä¼šè‡ªåŠ¨é€šè¿‡ config_class è§£æé…ç½®ï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†è¶…å‚æ•°ä¼ é€’ï¼Œç»Ÿä¸€ç®¡ç†æ¨¡å‹ç»“æ„ã€‚
    config_class = MiniMindConfig

    def __init__(self, config: MiniMindConfig = None):
        # 1. å¤„ç†é…ç½®ï¼šè‹¥æœªä¼ å…¥configï¼Œä½¿ç”¨é»˜è®¤çš„MiniMindConfig()
        self.config = config or MiniMindConfig()
        # 2. è°ƒç”¨çˆ¶ç±»PreTrainedModelçš„åˆå§‹åŒ–ï¼ˆå¿…é¡»ä¼ å…¥configï¼Œç”¨äºåç»­æƒé‡ç®¡ç†ï¼‰
        super().__init__(self.config)
        # 3. åˆå§‹åŒ–åŸºç¡€æ¨¡å‹ï¼ˆMiniMindModelï¼šæ ¸å¿ƒç¼–ç å™¨/è§£ç å™¨ï¼Œè´Ÿè´£æå–æ–‡æœ¬ç‰¹å¾ï¼‰
        self.model = MiniMindModel(self.config)
        # 4. åˆå§‹åŒ–è¯­è¨€å»ºæ¨¡å¤´ï¼ˆlm_headï¼‰ï¼šå°†ç‰¹å¾æ˜ å°„ä¸ºè¯æ±‡è¡¨æ¦‚ç‡åˆ†å¸ƒ
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)
        # 5. å…³é”®è®¾è®¡ï¼šè¯åµŒå…¥å±‚ä¸lm_headå…±äº«æƒé‡ã€å°†hiddenæ˜ å°„åˆ°è¯è¡¨å¤§å°çš„çº¿æ€§å±‚ä¸è¯åµŒå…¥å±‚å…±äº«æƒé‡ï¼Œå®ç°é«˜æ•ˆçš„è¯æ±‡è¡¨æ˜ å°„ã€‘
        #   è¯åµŒå…¥å±‚ï¼štoken ç´¢å¼• â†’ åµŒå…¥å‘é‡ï¼ˆç”¨å…±äº«æƒé‡çš„ã€Œè¡Œã€ï¼‰ï¼›
        #   lm_headï¼šç‰¹å¾å‘é‡ â†’ logitsï¼ˆç”¨å…±äº«æƒé‡çš„ã€Œåˆ—ã€ï¼‰ï¼›
        #   ä¸¤è€…å…±äº«æƒé‡ï¼Œä¿è¯ã€ŒåµŒå…¥â†’ç‰¹å¾â†’ç”Ÿæˆã€çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼ŒåŒæ—¶å‡å°‘å‚æ•°é‡ã€‚
        self.model.embed_tokens.weight = self.lm_head.weight
        # 6. åˆå§‹åŒ–è¾“å‡ºå®¹å™¨ï¼ˆå¯é€‰ï¼šå­˜å‚¨æ¨¡å‹è¾“å‡ºçš„ç»“æ„åŒ–å¯¹è±¡ï¼‰
        self.OUT = CausalLMOutputWithPast()

    def forward(self, 
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: Optional[bool] = False,
                # logitså¯ä»¥å­˜å‚¨ int æˆ–è€… tensor ç±»å‹
                # ã€Œä¿ç•™å‰ N ä¸ª logitsã€ or ã€ŒæŒ‰å¼ é‡ä¸­çš„ç´¢å¼•ä¿ç•™ logitsã€æ¯”å¦‚ä¼ å…¥å¼ é‡ torch.tensor([1,3,5])ï¼Œå°±æ˜¯ä¿ç•™ç¬¬ 1ã€3ã€5 ä¸ªä½ç½®çš„ logits
                logits_to_keep: Union[int, torch.Tensor] = 0,
                **kwargs):
        h, past_kvs, aux_loss = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            **kwargs
        )
        # ç›®çš„æ˜¯ã€Œç­›é€‰å‡ºéœ€è¦è®¡ç®— logits çš„ token ä½ç½®ã€ï¼Œé¿å…å¯¹æ•´ä¸ªåºåˆ—çš„ logits åšå†—ä½™è®¡ç®—ï¼ˆæ¯”å¦‚è®­ç»ƒæ—¶åªéœ€è¦é¢„æµ‹ã€Œä¸‹ä¸€ä¸ª tokenã€ï¼Œåªéœ€ä¿ç•™æœ€åä¸€ä¸ª token çš„ logitsï¼‰
        # æƒ…å†µ 1ï¼šlogits_to_keep æ˜¯ int ç±»å‹ï¼ˆå¦‚ 3ï¼‰ï¼š
        #   - slice(-3, None) ç­‰ä»·äºã€Œä»åºåˆ—å€’æ•°ç¬¬ 3 ä¸ªå…ƒç´ åˆ°æœ«å°¾ã€ï¼ˆPython çš„åˆ‡ç‰‡è¯­æ³•ï¼‰ï¼›
        #   - æ¯”å¦‚åºåˆ—é•¿åº¦ä¸º 5ï¼Œslice(-3, None) å¯¹åº”ç´¢å¼• [2,3,4]ï¼ˆä¿ç•™æœ€å 3 ä¸ª tokenï¼‰ã€‚
        # æƒ…å†µ 2ï¼šlogits_to_keep æ˜¯ torch.Tensor ç±»å‹ï¼ˆå¦‚ torch.tensor([1,3,5])ï¼‰ï¼š
        #   - ç›´æ¥å°†å¼ é‡ä½œä¸ºç´¢å¼•ï¼ˆéœ€ä¿è¯å¼ é‡æ˜¯æ•´æ•°ç±»å‹ï¼Œä¸”ç´¢å¼•åœ¨åºåˆ—é•¿åº¦èŒƒå›´å†…ï¼‰ï¼›
        #   - ç”¨äºç²¾å‡†ä¿ç•™ç‰¹å®šä½ç½®çš„ logitsï¼ˆå¦‚æŸäº›ä»»åŠ¡éœ€è¦é‡ç‚¹å…³æ³¨ä¸­é—´å‡ ä¸ª token çš„é¢„æµ‹ï¼‰ã€‚
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        # åªè®¡ç®—éœ€è¦çš„ logits
        # è¾“å…¥ï¼šh[:, slice_indices, :] â†’ ç­›é€‰åçš„ç‰¹å¾ï¼Œå½¢çŠ¶ä¸ºï¼š
        #   - è‹¥ logits_to_keep æ˜¯ int=3ï¼š[batch_size, 3, hidden_size]ï¼›
        #   - è‹¥ logits_to_keep æ˜¯å¼ é‡ [1,3,5]ï¼š[batch_size, 3, hidden_size]ï¼ˆ3 æ˜¯ç´¢å¼•ä¸ªæ•°ï¼‰ï¼›
        # å…³é”®ä¼˜åŒ–ï¼šåªå¯¹ã€Œéœ€è¦çš„ token ç‰¹å¾ã€åšçº¿æ€§æ˜ å°„ï¼Œé¿å…å¯¹æ•´ä¸ªåºåˆ—ï¼ˆæ¯”å¦‚é•¿åº¦ 1024ï¼‰çš„æ‰€æœ‰ token è®¡ç®— logitsï¼Œ
        # èŠ‚çœè®¡ç®—èµ„æºï¼ˆå°¤å…¶æ˜¯ç”Ÿæˆæ—¶ï¼Œåªéœ€è®¡ç®—æœ€åä¸€ä¸ª token çš„ logitsï¼‰ã€‚
        logits = self.lm_head(h[:, slice_indices, :])  
        self.OUT.__setitem__('last_hidden_state', h)
        self.OUT.__setitem__('logits', logits)
        self.OUT.__setitem__('aux_loss', aux_loss)
        self.OUT.__setitem__('past_key_values', past_kvs)
        return self.OUT